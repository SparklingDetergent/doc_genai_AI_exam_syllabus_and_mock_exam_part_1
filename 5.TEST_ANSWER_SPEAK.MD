---
**設問1**

畳み込みニューラルネットワーク (CNN) における「カーネル（フィルタ）」の主な役割として、最も適切なものはどれか。

ア) 入力データから特定の特徴（エッジ、テクスチャなど）を抽出する。
イ) 入力データの次元を削減し、計算量を減らす。
ウ) 活性化関数として機能し、非線形性を導入する。
エ) 出力層でクラスの確率を計算する。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ア）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

CNNっていうのは、特に画像を見るのが得意なAIの脳みそなんだけど、その中でも「カーネル」または「フィルタ」って呼ばれる部品が、めちゃくちゃ大事な仕事をしてるんだ。

「ア) 入力データから特定の特徴（エッジ、テクスチャなど）を抽出する。」これがカーネルの一番大事な役割！カーネルっていうのはね、小さな数字が並んだ板みたいなもの（例えば3x3マスとか）をイメージして。この板を、入力された画像の上で、少しずつスライドさせながら計算していくんだ（この計算を「畳み込み演算」って言うよ）。そうするとね、画像の中の「あっ、ここに縦線があるぞ！」とか「この辺りはザラザラした感じだな」とか「ここに赤くて丸いものがある！」みたいに、特定の部分的なパターン（特徴）にカーネルが「ピコーン！」って反応するんだ。その反応の結果をまとめたものが「特徴マップ」って呼ばれるもので、画像の中にどんな特徴がどこにあるかを示してるんだよ。

例えるなら、カーネルは特殊な模様が見える虫眼鏡みたいなものかな。その虫眼鏡で絵の隅々まで順番に見ていって、「あ、この模様（例えば赤い丸）見っけ！」ってなったら、別の紙（特徴マップ）に印をつけていく感じ。虫眼鏡の種類（カーネルの種類）を変えれば、また違う模様（例えば青い四角）を見つけることができるんだ。面白いよね！

他の選択肢も見てみようか。
「イ) 入力データの次元を削減し、計算量を減らす。」これは、主に「プーリング層」っていう別の部品のお仕事だね。カーネルの操作でちょっとサイズが変わることもあるけど、それがメインじゃないんだ。
「ウ) 活性化関数として機能し、非線形性を導入する。」活性化関数っていうのは、カーネルの計算が終わった後とかに登場する別の役者さんで、カーネル自体がその仕事をするわけじゃないんだ。
「エ) 出力層でクラスの確率を計算する。」「これは猫の確率何％」みたいに最終的な判断をするのは、主にAIの脳みその出口付近にいる「全結合層」とか「ソフトマックス関数」っていう部品たちの仕事だよ。

CNNではね、いろんな種類のカーネルが、AIが自分で学習していく中で自動的に作られていくんだ。そして、最初は単純な線とか模様を見つけるカーネルが、だんだん組み合わさって、もっと複雑な目や鼻、さらには顔全体みたいな特徴を見つけられるようになっていくんだ。すごい仕組みだよね！

---
**設問2**

CNNにおける「ストライド」の説明として、最も適切なものはどれか。

ア) 入力画像の外周に追加されるピクセルの幅。
イ) カーネルが一度に移動するピクセル数。
ウ) プーリング層で縮小される領域のサイズ。
エ) 畳み込み層の出力チャネル数。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

CNNで出てくる「ストライド」っていう言葉、これはね、カーネル君（さっき出てきた特徴を見つける虫眼鏡ね！）が、画像の上をどれくらいの「歩幅」で動いていくかを決める数字なんだ。

「イ) カーネルが一度に移動するピクセル数。」これがストライドのバッチリな説明！ストライドが「1」だったら、カーネルは画像の上を1ピクセルずつ、ちょこちょこ細かく動いていく。ストライドが「2」だったら、2ピクセルずつ、ちょっと大股で動いていく感じだね。

例えるなら、床をモップで拭くときを想像してみて。モップ（カーネル）を1マスずつ隣にずらしながら拭いていけば、それはストライド1。2マス飛ばしでずらしながら拭けば、ストライド2って感じ。ストライドを大きくすると、カーネルが見る場所が少なくなるから、結果として出来上がる特徴マップのサイズは小さくなるんだ。

他の選択肢も見てみよう！
「ア) 入力画像の外周に追加されるピクセルの幅。」これは「パディング」っていう別のテクニックの説明だね。画像の端っこって、カーネルを動かすと情報が途切れちゃうことがあるから、それを防ぐために、あらかじめ画像の周りに余白（パディング）を付けたりするんだ。
「ウ) プーリング層で縮小される領域のサイズ。」これは、プーリングっていう処理をするときに、どれくらいの範囲をギュッとまとめるかの大きさ（プールサイズとかウィンドウサイズって言うよ）の話だね。
「エ) 畳み込み層の出力チャネル数。」これは、何種類のカーネル（虫眼鏡）を使ったかによって決まる数だよ。カーネル1種類につき、1枚の特徴マップ（出力チャネル）ができるんだ。

ストライドの値を大きくしたり小さくしたりすることで、AIがどれくらい細かく画像の特徴を見るかとか、出来上がる特徴マップの大きさをコントロールできるんだ。これもAIの設計の大事なポイントの一つなんだね！

---
**設問3**

プーリング層の主な目的として、適切でないものはどれか。

ア) 入力データに対する位置の微小なずれに対して頑健性を持たせる（不変性の獲得）。
イ) 特徴マップの次元を削減し、計算コストを低減する。
ウ) ネットワークのパラメータ数を大幅に増やすことで、表現力を向上させる。
エ) 過学習を抑制する効果が期待できる場合がある。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ウ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「プーリング層」っていうのはね、CNNで畳み込み層（カーネル君が頑張るところね！）の後によく出てくる部品で、いくつか大事な役割があるんだ。この問題は「プーリング層の目的じゃないのはどれ？」って聞いてるね。

まず、プーリング層ができることを見てみよう！
「ア) 入力データに対する位置の微小なずれに対して頑健性を持たせる（不変性の獲得）。」これは大事な目的の一つ！例えば「最大値プーリング」っていうのは、ある範囲の中で一番大きな値だけを取り出すんだけど、そうすると、見つけたい特徴の位置がほんのちょっとズレてても、同じ一番大きな値が選ばれやすくなるんだ。だから、AIが「猫の耳がちょっと右にズレてるけど、まあ同じ耳だよね！」みたいに、ちょっとした位置の違いに強くなる（これを「不変性」って言うよ）。
「イ) 特徴マップの次元を削減し、計算コストを低減する。」これも大きな目的！プーリング処理をすると、特徴マップのサイズが小さくなるんだ。例えば、2x2の範囲から1つの値を選ぶと、縦も横も半分のサイズになる。そうすると、その後の計算がぐーんと楽になるし、コンピューターのメモリも節約できるんだ。
「エ) 過学習を抑制する効果が期待できる場合がある。」特徴マップのサイズが小さくなるってことは、AIが覚えないといけない数字（パラメータ）の数も間接的に減る効果があるんだ。そうすると、AIが訓練データのことばっかり覚えすぎちゃう「過学習」っていうのを防ぐのに役立つことがあるんだよ。

じゃあ、「ウ) ネットワークのパラメータ数を大幅に増やすことで、表現力を向上させる。」はどうかというと…これが適切じゃないんだな！プーリング層自体は、実は学習するパラメータ（AIが賢くなるために調整する数字）をほとんど持ってないんだ。だから、パラメータ数を「増やす」んじゃなくて、むしろ情報をギュッと「要約」することで、後続の層のパラメータ数を「減らす」方向に働くことが多いんだ。AIの表現力を上げるのは、主に畳み込み層とか全結合層とか、あとはAIの脳みそ全体の深さとか広さの役割なんだよね。

だから、プーリング層っていうのは、大事な情報をなるべく残しながら、情報をちょっと「荒く」することで、計算を楽にしたり、ちょっとした違いに強くなったり、過学習を防いだりする、縁の下の力持ちみたいな役割なんだって覚えておくといいよ！

---
**設問4**

ResNetで導入された「スキップ結合（ショートカット接続）」が主に解決しようとした問題は何か。

ア) 畳み込み処理の計算コストが高い問題。
イ) ネットワークの層が深くなることによる勾配消失問題や性能劣化。
ウ) プーリング層による情報損失の問題。
エ) 活性化関数の選択が難しい問題。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

ResNet（レズネット）っていうAIモデルで使われた「スキップ結合」または「ショートカット接続」っていうのはね、ディープラーニングの歴史の中で「これはスゴい発明だ！」って言われた、とっても大事なアイデアなんだ。

「イ) ネットワークの層が深くなることによる勾配消失問題や性能劣化。」これが、スキップ結合が解決しようとした一番大きな問題！理論的にはね、AIの脳みそ（ニューラルネットワーク）って、層をどんどんどんどん深く重ねていけば、もっともっと複雑なことも理解できて、性能も上がるはず！って思われてたんだ。でも、実際にやってみると、ある程度以上深くしちゃうと、なぜか学習がうまく進まなくなっちゃう。具体的には、AIが学習するための「ヒント」（勾配って言うよ）が、脳みその奥の方までちゃんと届かなくなっちゃう「勾配消失問題」とか、トレーニングの成績も本番の成績も悪くなっちゃう「劣化問題」っていうのが起きて、研究者たちを悩ませてたんだ。

そこでResNetが考え出したのが、このスキップ結合！これはね、AIの脳みその中で、情報の流れに「近道（ショートカット）」を作ってあげるんだ。いくつかの層をピューンと飛び越えて、入ってきた情報を直接、先の層の出力に足し合わせちゃう。こうすることで、学習のヒント（勾配）が、この近道を通って脳みその奥の方までスムーズに届きやすくなる。それに、AIが少なくとも「入ってきた情報をそのまま出す」っていう簡単なこと（これを「恒等写像」って言うよ）は学習しやすくなるから、うーんと深い脳みそを作っても、学習がちゃんと進んで、性能も悪くなりにくくなったんだ！

例えるなら、ものすごーく高いタワー（深いネットワーク）を建てるときに、各階停まりのエレベーター（通常の情報の流れ）だけじゃなくて、特定の階まで一気に上がれる急行エレベーター（スキップ結合）も作った感じかな。急行エレベーターがあれば、上の階（深い層）にも情報が楽に届くでしょ？そんなイメージだよ。

他の選択肢も見てみよう！
「ア) 畳み込み処理の計算コストが高い問題。」計算コストを減らすのは、プーリング層とかストライドとか、あとはモデルを軽くする技術のお仕事で、スキップ結合の主な目的じゃないんだ。
「ウ) プーリング層による情報損失の問題。」プーリングで情報がちょっと失われるのは確かだけど、スキップ結合が直接それを解決するわけじゃないんだ。
「エ) 活性化関数の選択が難しい問題。」どんな活性化関数を選ぶかは大事だけど、スキップ結合はまた別の問題を解決しようとしてるんだ。

ResNetのスキップ結合のおかげで、それまでは難しかった何百層、何千層っていう、ものすごーく深いAIの脳みそを作れるようになって、画像認識とかの分野でAIの性能がめちゃくちゃ上がったんだ。まさに革命的なアイデアだったんだね！

---
**設問5**

リカレントニューラルネットワーク (RNN) が特に適しているデータの種類は何か。

ア) 画像データのようなグリッド構造を持つデータ。
イ) 顧客の属性データのようなテーブル形式のデータ。
ウ) 音声やテキストのような時系列データやシーケンシャルデータ。
エ) 特徴間の関係性が複雑な高次元の疎なデータ。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ウ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

RNN（アールエヌエヌって読むよ）、正式名称はリカレントニューラルネットワーク！このAIの脳みそはね、データの「順番」とか「時間の流れ」みたいなのを扱うのが得意なんだ。

「ウ) 音声やテキストのような時系列データやシーケンシャルデータ。」これがRNNが一番得意なデータのタイプ！RNNはね、脳みその中に「隠れ状態」っていう、過去の情報を一時的に記憶しておくためのループ構造を持ってるんだ。だから、今の情報を見るときに、「さっき何があったっけな？」っていう過去の情報を思い出しながら処理できる。これって、単語が順番に並んでる文章（テキスト）とか、音が連続してる話し声（音声）、株価の上がり下がり（時系列データ）みたいに、順番に意味があるデータを理解するのにピッタリだよね！

例えば、みんなが本を読むときって、前の単語の意味を覚えながら次の単語を読んで、文全体の意味を理解していくでしょ？RNNもそれに似たようなことをしてるんだ。「私は 学生 です。」っていう文を読むとき、「私」「は」「学生」…って順番に単語を処理しながら、それぞれの単語の意味を考えつつ、文全体の意味を掴もうとするんだ。

他の選択肢も見てみようか。
「ア) 画像データのようなグリッド構造を持つデータ。」画像みたいに、マス目が並んでるデータは、主にCNN（畳み込みニューラルネットワーク）っていう、空間の広がりを見るのが得意なAIの脳みそが得意な分野だね。でもね、画像に写ってるものを説明する文章を作る（画像キャプション生成って言うよ）みたいに、CNNで画像の特徴を読み取った後に、RNNでその特徴を元に文章を作り出す、なんていう合わせ技もあるんだよ！
「イ) 顧客の属性データのようなテーブル形式のデータ。」お客さんの年齢とか性別とかが並んでる表みたいなデータは、普通のニューラルネットワーク（全結合型って言うよ）とか、決定木っていう種類のアルゴリズム（ランダムフォレストとか勾配ブースティングとか）がよく使われるね。データの順番があんまり関係ない場合は、RNNの良さが活きにくいんだ。
「エ) 特徴間の関係性が複雑な高次元の疎なデータ。」これだけじゃ、RNNが一番いいとは言えないな。どんなデータで、何をしたいかによるね。

だから、RNNっていうのは、その「記憶力」のおかげで、過去の出来事が未来に影響を与えるような、時間と順番が大事なデータを扱うのに、とっても頼りになるAIの脳みそなんだ！

---
**設問6**

LSTM (Long Short-Term Memory) や GRU (Gated Recurrent Unit) が、単純なRNNと比較して改善した主な点は何か。

ア) 計算速度を大幅に向上させた。
イ) より少ないデータで学習できるようになった。
ウ) 長期的な依存関係を学習しやすくなり、勾配消失問題を緩和した。
エ) あらゆる種類のデータに対応できるようになった。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ウ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

LSTM（エルエスティーエム）とかGRU（ジーアールユー）っていうのはね、さっき出てきたRNNの仲間なんだけど、普通のRNN（Simple RNNって言ったりするよ）がちょっと苦手だったことを克服するために考え出された、もっとパワーアップしたRNNなんだ。

「ウ) 長期的な依存関係を学習しやすくなり、勾配消失問題を緩和した。」これがLSTMやGRUが一番「グッジョブ！」な改善ポイント！普通のRNNってね、あんまり昔のことまで覚えてられないっていう弱点があったんだ。文章で言うと、すごーく長い文章の最初の方に出てきた大事な言葉の意味を、最後まで覚えておくのが苦手だった。これはね、AIが学習するときの「ヒント」（勾配）が、昔の情報までうまく伝わらなくなっちゃう「勾配消失問題」とか、逆に情報が爆発しちゃう「勾配爆発問題」が原因の一つだったんだ。

そこでLSTMやGRUが考えたのが、「ゲート」っていう特別な門番みたいな仕組み！このゲートがね、過去の情報を見て「これは大事だから覚えておこう！」「これはもういらないから忘れちゃおう！」「新しい情報、これを追加しておこう！」みたいに、情報の流れを賢くコントロールしてくれるんだ。おかげで、本当に必要な情報だけを長ーい間しっかり覚えておけるようになって、勾配消失問題も起こりにくくなったんだ！

例えるなら、普通のRNNが、聞いたことをすぐに忘れちゃう短期記憶しか持ってない人だとすると、LSTMやGRUは、大事なことだけを選んで、ちゃんと長期記憶に保存できる「賢いメモ帳」（これがゲート機構ね！）を持ってる人みたいな感じかな。だから、長い文章の最初に出てきたキーワードの意味を、文の終わりまでちゃんと覚えておけるようになったんだ。すごいよね！

他の選択肢も見てみよう。
「ア) 計算速度を大幅に向上させた。」うーん、LSTMとかGRUって、普通のRNNより中の仕組みがちょっと複雑になってるから、計算にかかる時間はむしろ増えちゃうことが多いんだ。スピードアップがメインの改善点じゃないんだよね。
「イ) より少ないデータで学習できるようになった。」お手本データの量はやっぱり大事で、LSTMやGRUだからって、ものすごく少ないデータでOK！ってわけじゃないんだ。
「エ) あらゆる種類のデータに対応できるようになった。」LSTMやGRUもRNNの仲間だから、得意なのはやっぱり言葉とか音みたいな、順番が大事なデータだね。対応できるデータの種類がガラッと変わったわけじゃないんだ。

LSTMやGRUが出てきたおかげで、AIはもっと長い話の流れを理解したり、もっと複雑な時間のパターンを学習したりできるようになって、機械翻訳とか、音声認識、文章作りみたいな分野が、ものすごく進歩したんだよ！

---
**設問7**

Transformerモデルで中心的な役割を果たす「Attention機構」に関する説明として、最も適切なものはどれか。

ア) 入力シーケンスの各要素に対して、均等な重みを割り当てることで情報を集約する。
イ) 出力を生成する際に、入力シーケンスの特定の部分に注目し、関連性の高い情報に大きな重みを割り当てる。
ウ) ネットワークの層をスキップすることで、情報の伝達を効率化する。
エ) 畳み込み演算を用いて、局所的な特徴を抽出する。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「Attention（アテンション）機構」、日本語だと「注意機構」なんて言ったりするんだけど、これはね、特に「Transformer（トランスフォーマー）」っていうAIモデルの大成功で、一気に有名になった、ディープラーニングの世界でめちゃくちゃ大事な考え方なんだ。

「イ) 出力を生成する際に、入力シーケンスの特定の部分に注目し、関連性の高い情報に大きな重みを割り当てる。」これがAttention機構のど真ん中のアイデア！例えば、AIに長ーい文章を読ませて何かをさせるとき、その文章の中の全ての言葉が同じくらい重要ってわけじゃないよね？答えを出すのに特に大事な部分と、そうでもない部分があるはず。Attention機構っていうのは、AIが何か答えを出そうとするときに、入力された文章の「どの部分に注目したらいいかな？」っていうのを、その都度自動的に学習して、特に関連が深い部分の情報に「ここが大事！」って感じで大きな重みをつけて使う仕組みなんだ。

例えるなら、君が英語の長文を読んで日本語に訳すときを想像してみて。日本語の一つの単語を訳そうとするとき、英文全体の全ての単語をぼんやり眺めるんじゃなくて、「あ、この日本語の単語は、英文のこの辺りの単語と強く関係してるな！」って感じで、特定の場所に「注目」するでしょ？Attention機構は、AIにそういう「どこに注目すべきか」を自動で見つけさせる賢い仕組みなんだ。

他の選択肢も見てみようか。
「ア) 入力シーケンスの各要素に対して、均等な重みを割り当てることで情報を集約する。」これは、全部の情報を同じように混ぜこぜにする感じだから、Attentionの「特定の場所に注目する」っていうのとは違うね。
「ウ) ネットワークの層をスキップすることで、情報の伝達を効率化する。」これは、さっき出てきたResNetの「スキップ結合」の説明だね。Attentionとは別物だよ。
「エ) 畳み込み演算を用いて、局所的な特徴を抽出する。」これはCNN（畳み込みニューラルネットワーク）の説明だね。Attention機構は、必ずしも隣り合った部分だけじゃなくて、もっと離れた場所にある情報同士の関連性も見つけ出すことができるんだ。

Transformerモデルっていうのは、このAttention機構をこれでもか！ってくらい全面的に使ってて、それまでのRNN（リカレントニューラルネットワーク）を使わなくても、コンピューターがいっぺんに計算できる（並列計算って言うよ）ようにしつつ、しかも言葉の長ーい繋がりもちゃんと理解できるっていう、すごいことを成し遂げたんだ。だから、今の自然言語処理（人間の言葉をAIが扱う技術）の世界に、まさに革命を起こしたって言われてるんだよ！「自己注意（Self-Attention）」なんて言葉も聞いたことあるかな？これもTransformerの大事な部品だよ！

---
**設問8**

データ拡張 (Data Augmentation) の主な目的として、最も適切なものはどれか。

ア) 学習データの量を擬似的に増やし、モデルの過学習を抑制して汎化性能を向上させる。
イ) 学習データのノイズを除去し、クリーンなデータセットを作成する。
ウ) 学習データの次元を削減し、計算効率を高める。
エ) モデルの解釈性を向上させるために、重要な特徴を強調する。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ア）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「データ拡張（データオーグメンテーションとも言うよ）」っていうのはね、手元にあるAIのお手本データ（学習データ）を、もっともっと有効に活用して、AIの性能をアップさせるための、とってもポピュラーなテクニックなんだ。

「ア) 学習データの量を擬似的に増やし、モデルの過学習を抑制して汎化性能を向上させる。」これがデータ拡張の一番のねらい！AIモデル、特にディープラーニングのモデルって、賢くなるためには、たーくさんのお手本データが必要なんだ。でも、いつもいつも十分な量のデータを集められるとは限らないよね？そんなときに役立つのがデータ拡張！これはね、既にあるお手本データに、ほんのちょっとずつ変化を加えた「新しい」データを、人工的にいっぱい作り出すんだ。そうすることで、AIは実質的にたくさんの種類のデータで勉強できることになる。結果として、AIが訓練データのことばっかり覚えすぎちゃう「過学習」っていうのを防いで、まだ見たことのない新しいデータに対してもちゃんと答えを出せる能力（これを「汎化性能」って言うよ）を高めることができるってわけ！

例えばね、AIに猫の画像を覚えさせたいとするでしょ？そしたら、元の猫の画像に、ちょっとだけ回転を加えたり、左右をひっくり返したり、明るさをちょっと変えてみたり、画像の一部をズームアップしてみたり…そんな風に加工した画像を、新しいお手本データとして追加するんだ。これらは全部「猫」であることには変わりないけど、見た目は少しずつ違うから、AIは「あ、猫ってこういういろんな姿があるんだな！」ってもっとたくさんのことを学べるんだ。賢いやり方だよね！

他の選択肢も見てみよう！
「イ) 学習データのノイズを除去し、クリーンなデータセットを作成する。」お手本データから余計なものを取り除く「データクリーニング」とか「ノイズ除去」も大事な作業だけど、データ拡張の主な目的とはちょっと違うんだ。むしろデータ拡張は、AIが多少のノイズとか見た目の変化に強くなる（ロバストになるって言うよ）効果も期待できるんだ。
「ウ) 学習データの次元を削減し、計算効率を高める。」データの情報をギュッと圧縮する「次元削減」（主成分分析とかが有名だね）とは、目的が違うね。
「エ) モデルの解釈性を向上させるために、重要な特徴を強調する。」AIがどうしてそういう答えを出したのか分かりやすくする「解釈性向上」（XAIとか）とは、直接の目的じゃないんだ。

データ拡張は、比較的簡単に試せて、多くの場合でAIの性能アップにつながるから、実際のAI開発の現場でもよく使われるテクニックなんだ。どんな風にデータを「拡張」するのが一番効果的かは、データの種類とか、AIに何をさせたいかによって変わってくるから、そこは工夫のしどころだね！

---
