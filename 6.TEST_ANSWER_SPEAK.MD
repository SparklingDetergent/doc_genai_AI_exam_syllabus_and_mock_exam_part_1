---
**設問1**

大規模言語モデル (LLM) に関する記述として、最も適切なものはどれか。

ア) 主に画像認識タスクのために設計された小規模なニューラルネットワークである。
イ) 大量のテキストデータで事前学習され、人間のような自然なテキストを生成したり、理解したりする能力を持つ。
ウ) 強化学習のみを用いて学習され、特定のゲームを攻略する能力に特化している。
エ) 音声認識と音声合成を組み合わせることで、リアルタイムの翻訳を行う。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「大規模言語モデル」、略してLLM！最近ニュースとかでもよく聞くようになったよね？これ、AIの世界で今めちゃくちゃホットな話題なんだ。

「イ) 大量のテキストデータで事前学習され、人間のような自然なテキストを生成したり、理解したりする能力を持つ。」これがLLMのドンピシャな説明！LLMっていうのはね、インターネットにあるウェブサイトの文章とか、たくさんの本とか、とにかくものすごーーーい量の文字データ（テキストデータって言うよ）を、AIに「これでもか！」ってくらい読ませて勉強させるんだ。そうするとね、AIは人間が使う言葉の言い回しとか、文法のルールとか、さらには話の流れ（文脈って言うよ）に応じて「こういう時はこういう意味だよね」とか「こういう風に文章を作ると自然だよね」っていうのを、だんだん理解して、自分で文章を作ったりできるようになるんだ。OpenAIのGPTシリーズとか、GoogleのBERTとかLaMDAとか、聞いたことあるかな？あれがLLMの代表選手だよ。

例えるなら、人間が小さい頃からたくさんの本を読んだり、人と話したりして言葉を覚えていくみたいに、LLMも大量の文章を読むことで、「あ、この単語の後には、この単語が来やすいんだな」とか「こういう質問をされたら、こういう風に答えるのが普通だな」っていうのを学習していくんだ。だから、質問に答えたり、新しい文章を考えたり、外国語に翻訳したり、長い文章を短くまとめたり…言葉に関することなら、いろんなことができるようになるんだよ。すごくない？

他の選択肢も見てみようか。
「ア) 主に画像認識タスクのために設計された小規模なニューラルネットワークである。」LLMは、主に言葉（テキスト）を扱うのが得意で、その名前の通り「大規模」なAIの脳みそ（ニューラルネットワーク）なんだ。脳みその部品（パラメータって言うよ）が何億個、いや何兆個もあるような、超巨大なやつもあるんだよ。画像を見るのが得意なのは、CNNっていう別のタイプのAIだね。
「ウ) 強化学習のみを用いて学習され、特定のゲームを攻略する能力に特化している。」LLMの勉強方法は、主に「自己教師あり学習」っていって、大量のテキストデータそのものからAIが自分で「これが問題で、これが答えかな？」って見つけて学んでいくやり方なんだ。もちろん、特定の仕事にもっと強くなるために「ファインチューニング」したり、人間が「その答え方、いいね！」とか「それはちょっと…」ってフィードバックして、もっと人間に役立つように教え込む「強化学習（RLHFって言うよ）」も使われたりするけど、「強化学習だけ」ってわけじゃないし、ゲーム専門でもないんだ。
「エ) 音声認識と音声合成を組み合わせることで、リアルタイムの翻訳を行う。」声を聞き取ったり、声を作り出したりするのもAIの大事な技術だけど、LLMのメインの仕事はやっぱり言葉を理解したり作ったりすることなんだ。翻訳システムでLLMが中心的な役割を果たすことは多いけど、声のやり取りはまた別の技術とチームを組んでやってるんだよ。

LLMはね、いろんなことに応用できるから、「基盤モデル（ファウンデーションモデル）」なんて呼ばれたりもして、これからますますいろんなところで活躍しそうだね！楽しみだね！

---
**設問2**

Diffusion Model (拡散モデル) が近年注目されている主な応用分野は何か。

ア) 表形式データの回帰分析
イ) 高品質な画像や音声などのデータ生成
ウ) 時系列データの異常検知
エ) ソフトウェアのバグ検出

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「Diffusion Model（ディフュージョンモデル）」、日本語だと「拡散モデル」って言うんだけど、これも最近AIの世界で「なんかすごいのが出てきたぞ！」って話題になってる、新しいタイプの「もの作りAI」（生成モデルって言うよ）なんだ。

「イ) 高品質な画像や音声などのデータ生成」これがDiffusion Modelが一番得意なこと！どういう仕組みかっていうとね、まず元のキレイなデータ（例えば、写真とか音楽とか）に、わざとちょっとずつノイズ（砂嵐みたいなザラザラしたやつ）を加えていって、最後にはもう何が何だか分からない完全なノイズにしちゃうんだ（これを「拡散過程」って言うよ）。で、AIにはその逆のプロセス、つまりノイズだらけの状態から、ちょっとずつノイズを取り除いていって、元のキレイなデータに戻す方法（これを「逆拡散過程」とか「生成過程」って言うよ）を学習させるんだ。学習が終わると、AIは全くのランダムなノイズからスタートして、まるで彫刻家が石から像を掘り出すみたいに、ノイズを丁寧に取り除いていくことで、本物そっくりの新しいデータを生み出すことができるようになるんだ！

例えるなら、ピカピカの窓ガラスに、少しずつ霧吹きで水をかけていって、最後は真っ白で何も見えない状態にする（これが拡散過程）。次に、その真っ白な窓ガラスを、今度は少しずつ丁寧に拭いていくと、だんだん元の景色が見えてくるよね？（これが生成過程）。Diffusion Modelは、この「霧を晴らして景色を再現する」方法を学習して、最後には何もないところから、まるで本物みたいな新しい景色（データ）を描き出せるようになる、そんなイメージなんだ。最近よく聞く「Stable Diffusion」とか「DALL-E 2」（の一部）っていう、すごい絵を描くAIも、このDiffusion Modelの技術を応用してるんだよ。

他の選択肢も見てみよう。
「ア) 表形式データの回帰分析」これは、数字のデータを扱って、将来の数値を予測するような問題だから、Diffusion Modelの主な出番じゃないね。
「ウ) 時系列データの異常検知」時間の流れに沿ったデータで「あれ？いつもと違うぞ！」っていうのを見つける問題だね。RNNとかTransformerが得意な分野だけど、Diffusion Modelがメインで使われるわけじゃないかな（研究レベルでは面白い試みがあるかもしれないけどね！）。
「エ) ソフトウェアのバグ検出」これはコンピュータープログラムの間違いを見つける仕事だから、今のDiffusion Modelが主に活躍してる場所とは違うね。

Diffusion Modelはね、それまでの「もの作りAI」（例えばGANとか）と比べて、学習が比較的安定しやすくて、しかもめちゃくちゃリアルで、いろんな種類のデータを上手に作れる可能性があるってことで、絵だけじゃなくて、音楽を作ったり、動画を作ったり、いろんなクリエイティブな分野で「これは使えるぞ！」って期待されてるんだ。ワクワクするよね！

---
**設問3**

「Zero-shot Learning」の説明として、最も適切なものはどれか。

ア) 学習データに全く含まれていない新しいクラスのサンプルに対しても、ある程度の識別や理解ができる学習手法。
イ) 非常に少数の学習サンプルだけを用いて、新しいタスクに対応する学習手法。
ウ) 複数の異なるモダリティのデータ（例：画像とテキスト）を同時に処理し、理解する学習手法。
エ) モデルのパラメータ数をゼロに近づけることで、極めて軽量なモデルを作成する手法。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ア）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「Zero-shot Learning（ゼロショットラーニング）」、略してZSL！これはね、AIが「えっ、これ見たことないけど、たぶんアレだ！」みたいに、一度も習ったことがない新しい種類のものを、なんとか当てようとする、とっても野心的な学習方法なんだ。

「ア) 学習データに全く含まれていない新しいクラスのサンプルに対しても、ある程度の識別や理解ができる学習手法。」これがZero-shot Learningのバッチリな説明！普通のAI（教師あり学習って言うよ）は、トレーニングの時に見たことがあるものしか分からないんだ。「犬」と「猫」の写真ばっかり見て勉強したAIは、「犬」と「猫」は分かるけど、いきなり「馬」の写真を見せられても「？？？」ってなっちゃう。でもZSLは違うんだ！例えば、「シマウマ」っていう動物の写真を一度も見たことがなくても、「馬に似てる動物で、体に縞模様があるんだよ」っていう情報（これを「属性情報」とか「意味情報」って言うよ）をあらかじめ教えておけば、初めてシマウマの画像を見たときに「うーん、これは馬っぽくて、しかも縞々だ…もしかしてシマウマじゃないか！？」って推測できるようになるかもしれない。そんな賢いことを目指してるんだ。

例えるなら、子供に「パンダ」っていう動物の写真や絵を一度も見せたことがなくても、「白と黒のブチ模様で、クマさんみたいに大きくて、笹の葉を食べる動物だよ」って言葉で教えておけば、動物園で初めてパンダを見たときに「あ！あれがパンダかな？」って分かるかもしれないよね。ZSLは、AIにそういう人間みたいな柔軟な認識力を持たせようとしてるんだ。

他の選択肢も見てみよう。
「イ) 非常に少数の学習サンプルだけを用いて、新しいタスクに対応する学習手法。」これは「Few-shot Learning（フューショットラーニング）」っていう別の学習方法の説明だね。ほんの数枚のお手本写真だけで新しいものを覚えさせようとするやつ。ZSLは「ゼロ」ショットだから、新しいもののお手本写真は一枚もないんだ。
「ウ) 複数の異なるモダリティのデータ（例：画像とテキスト）を同時に処理し、理解する学習手法。」これは「マルチモーダル学習」っていう、画像と言葉とか、音と映像みたいに、いろんな種類の情報を一緒に扱うAIの説明だね。ZSLで、さっき言ったみたいに言葉の情報を手がかりにすることはあるけど、ZSLそのものの定義じゃないんだ。
「エ) モデルのパラメータ数をゼロに近づけることで、極めて軽量なモデルを作成する手法。」これは、AIの脳みそをダイエットさせて、スマホとかでも動くように軽くする技術（モデル圧縮とか軽量化って言うよ）の話で、ZSLとは関係ないね。

Zero-shot Learningは、AIがもっと人間みたいに、知らないことでも今までの知識を総動員してなんとか理解しようとする、そんな賢さを身につけるための、とっても大事な研究テーマなんだ。これがもっと進化したら、AIがもっといろんな場面で活躍できるようになるかもしれないね！

---
**設問4**

「転移学習 (Transfer Learning)」の主な利点として、適切でないものはどれか。

ア) あるタスクで学習した知識を別の関連タスクに活用することで、学習に必要なデータ量を削減できる。
イ) 大規模なデータセットで事前学習されたモデルを利用することで、学習時間を短縮できる。
ウ) ターゲットタスクにおけるモデルの性能向上が期待できる。
エ) あらゆるタスクにおいて、常にゼロから学習するよりも高い性能を保証する。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（エ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「転移学習（てんいがくしゅう）」っていうのはね、あるお仕事（タスク）でAIが一生懸命勉強して身につけた知識やコツを、別の、でもちょっと似てるお仕事に「転校」させて、再利用するっていう賢い学習テクニックなんだ。

転移学習のいいところ、いっぱいあるんだよ！
「ア) あるタスクで学習した知識を別の関連タスクに活用することで、学習に必要なデータ量を削減できる。」これは大きなメリット！例えば、ものすごーくたくさんの普通の写真（犬とか猫とか車とか）を見て「物の形ってこういうものだよね」って学習したAI（これを「事前学習済みモデル」って言うよ）がいるとするでしょ？そのAIの知識を、今度はちょっと専門的な写真（例えば、珍しい蝶の種類を見分けるとか、お医者さんが見るレントゲン写真とか）を覚えるのに使うんだ。そうすると、専門的な写真のお手本データがあんまりたくさんはなくても、AIは「あ、基本的な物の見方はもう知ってるから、新しい蝶の特徴だけ覚えればいいんだな！」って感じで、比較的少ないデータでも賢くなれることがあるんだ。便利だよね！
「イ) 大規模なデータセットで事前学習されたモデルを利用することで、学習時間を短縮できる。」これも嬉しいポイント！事前学習済みモデルは、もうすでに基本的なことはいっぱい勉強してきてる「優等生」みたいなものだから、新しいお仕事を覚えるのも早いんだ。ゼロから全部勉強し直すよりも、ずっと短い時間で目標の性能に到達できることが多いよ。
「ウ) ターゲットタスクにおけるモデルの性能向上が期待できる。」これも大事なメリット！特に、新しいお仕事のお手本データが少ないときなんかは、ゼロから勉強するよりも、関連する分野でいっぱい勉強してきたAIの知識を使った方が、もっと応用力が高い、つまり「本番に強い」AIになることが期待できるんだ。

じゃあ、「エ) あらゆるタスクにおいて、常にゼロから学習するよりも高い性能を保証する。」はどうかというと…これが「適切でない」んだな。「常に保証する」ってわけじゃないのがポイント。転移学習がうまくいくためには、元の仕事と新しい仕事がある程度「似てる」とか「関係がある」っていうのが大事なんだ。全然関係ない仕事の間で知識を無理やり転校させようとすると、かえってAIが混乱しちゃって性能が悪くなる「負の転移」っていう現象が起きちゃうこともあるんだ。それに、もし新しいお仕事のお手本データがものすごーくたくさんあって、しかも質もめちゃくちゃ良いんだったら、最初からそのデータだけでみっちり勉強した方が、いいAIができる可能性だってあるからね。

だから、転移学習はAI開発をうんと効率的にしてくれて、性能も上げてくれることが多い、とっても便利なテクニックだけど、「万能薬じゃないよ」ってことは覚えておこうね！

---
**設問5**

モデルの解釈性 (XAI: Explainable AI) が重要視される理由として、最も適切なものはどれか。

ア) モデルの予測精度を最大化するため。
イ) モデルの学習に必要な計算コストを削減するため。
ウ) モデルの判断根拠を人間が理解できるようにし、信頼性や透明性を確保するため。
エ) モデルのアーキテクチャを自動的に最適化するため。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ウ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

XAI（エックスエーアイって読むよ）、これは「Explainable AI」の略で、日本語だと「説明可能なAI」なんて言われるんだ。AIがどうしてそういう答えを出したのか、その考え方のプロセスとか理由を、人間にもちゃんと分かるようにしよう！っていう技術や考え方のことなんだ。

「ウ) モデルの判断根拠を人間が理解できるようにし、信頼性や透明性を確保するため。」これが、XAIがすごく大事だって言われてる一番の理由！特にディープラーニングみたいな複雑なAIって、中で何がどうなってその答えが出てきたのか、人間にはさっぱり分からないことが多くて、「ブラックボックス」なんて言われちゃうこともあるんだ。XAIは、この真っ黒な箱の中身をちょっと覗けるようにしたり、「AIさんはこういう理由でこう判断したんですよ」って人間にも分かる言葉で説明してくれたりすることを目指してるんだ。

なんでそんなことが大事かっていうとね…
まず、「信頼性アップ！」AIがどうしてそう考えたのか分かれば、「うん、その理由なら納得だね！」って人間もAIのことを信頼して使えるようになるよね？
次に、「公平性チェック！」AIが「この人は採用！この人は不採用！」みたいな判断をするときに、もしかしたら性別とか出身地とか、そういうことで不公平な判断をしてないか、ちゃんとチェックできる。
そして、「安全性確認！」お医者さんが使うAIとか、自動運転のAIとか、人の命に関わるような場面では、AIが間違った判断をしたら大変なことになるでしょ？だから、どうしてそういう判断をしたのかを理解することが、安全のためにも絶対に必要だよね。
他にも、AIが間違っちゃったときに、原因を見つけて直すのにも役立つし（デバッグや改善）、もしAIの判断で何か問題が起きたときに、誰の責任なのかをはっきりさせるためにも（法的・倫理的責任）、説明できるってことがすごく大事なんだ。

他の選択肢も見てみようか。
「ア) モデルの予測精度を最大化するため。」XAIの主な目的は、AIの答えの正確さを上げることじゃないんだ。むしろ、分かりやすくするために、AIの複雑さをちょっと抑えたり、説明のための特別な処理が必要になったりすることもあるくらいだよ。
「イ) モデルの学習に必要な計算コストを削減するため。」XAIの技術を使うことで、むしろ余計に計算が必要になることもあるんだ。
「エ) モデルのアーキテクチャを自動的に最適化するため。」AIの脳みその形を自動で一番いい感じにしてくれるのは、「ニューラルアーキテクチャサーチ (NAS)」とか、そういう別の技術の話だね。

AIがこれからもっともっと社会のいろんな場面で使われるようになるためには、みんながAIのことをちゃんと信頼できて、安心して任せられるようにならないといけないよね。だから、このXAIっていう考え方が、ますます重要になってきてるんだ！

---
**設問6**

モデルの軽量化技術の一つである「量子化 (Quantization)」とは、どのような手法か。

ア) モデルの重要でない接続を削除（プルーニング）することで、パラメータ数を削減する手法。
イ) 大きなモデル（教師モデル）の知識を、より小さなモデル（生徒モデル）に転移させる手法。
ウ) モデルの重みや活性化関数を、より少ないビット数の数値表現に変換することで、モデルサイズと計算量を削減する手法。
エ) 複数の異なるモデルの予測を組み合わせることで、単一モデルよりも高い性能を得る手法。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ウ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

AIモデル、特にディープラーニングのモデルって、賢いんだけど、その分、脳みそ（モデル）がものすごく大きくて重たくなりがちなんだ。そうすると、スマホとか、ちっちゃなコンピューター（エッジデバイスって言うよ）で動かすのが大変になっちゃう。そこで登場するのが「モデルの軽量化」っていう、AIのダイエット技術！「量子化（りょうしか）」はそのダイエット方法の中でも代表的なものの一つなんだ。

「ウ) モデルの重みや活性化関数を、より少ないビット数の数値表現に変換することで、モデルサイズと計算量を削減する手法。」これが量子化のバッチリな説明！普通ね、AIの脳みそ中の数字（重みとか活性化関数の値）って、32ビット浮動小数点数っていう、結構細かーい精度で表されてるんだ。量子化っていうのは、この細かーい数字を、例えば8ビット整数とか、もっと少ないビット数の大雑把な数字に「えいやっ！」って変換しちゃうことなんだ。

そうすると、どんないいことがあるかっていうとね…
まず、「モデルが小さくなる！」一つの数字を表すのに使うビットの数が減るから、AIの脳みそ全体のファイルサイズがギューッと小さくなるんだ。ダイエット成功だね！
次に、「計算が速くなる！」ビット数が少ない数字の計算って、ビット数が多い浮動小数点数の計算よりも、コンピューターが得意なことが多いんだ。だから、AIが答えを出すスピード（推論速度って言うよ）が速くなる。おまけに、使う電気の量も減らせることがあるんだ。エコだね！

例えるなら、アナログの体重計って、メモリが細かくて連続的に体重が分かるけど、ちょっと読み取りにくいこともあるよね？それを、1キログラム刻みのデジタル表示の体重計に変えるようなイメージかな。体重のほんのちょっとした変化は分からなくなっちゃうかもしれないけど（多少の精度は犠牲になるかも）、表示はスッキリするし、パッと見て分かりやすくなるでしょ？そんな感じ！

他の選択肢も見てみよう！
「ア) モデルの重要でない接続を削除（プルーニング）することで、パラメータ数を削減する手法。」これは「プルーニング」または「枝刈り」っていう、AIの脳みそで「この神経、あんまり仕事してないな…」っていう部分をバッサリ切っちゃう、別のダイエット方法だね。
「イ) 大きなモデル（教師モデル）の知識を、より小さなモデル（生徒モデル）に転移させる手法。」これは「知識蒸留（ちしきじょうりゅう）」っていう、大きな賢いAI先生の知識を、小さな生徒AIにギュッと凝縮して教え込む、別のダイエット兼お勉強方法だね。
「エ) 複数の異なるモデルの予測を組み合わせることで、単一モデルよりも高い性能を得る手法。」これは「アンサンブル学習」っていう、一人で頑張るよりチームで頑張った方がいい結果が出るよね！っていう、性能アップのためのテクニックだね。

量子化はね、AIの賢さをできるだけ保ちつつ、体を小さく軽くしようとするんだけど、やっぱりちょっとは大雑把になっちゃうから、少しだけ賢さが落ちちゃうこともあるんだ。そのバランスをうまく取るのが、腕の見せ所なんだね！

---
**設問7**

「マルチモーダルAI」が扱うデータの例として、最も適切なものはどれか。

ア) 大量の英語のテキストデータのみ。
イ) 様々な犬種の画像データのみ。
ウ) 画像とその画像内容を説明するテキストのペアデータ。
エ) 金融市場の株価の時系列データのみ。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ウ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「マルチモーダルAI」って、なんだかカッコいい名前だよね！「モーダル」っていうのは「情報の種類」みたいな意味で、「マルチ」は「複数」ってことだから、つまり「いろんな種類の情報をいっぺんに扱えるAI」ってことなんだ。

「ウ) 画像とその画像内容を説明するテキストのペアデータ。」これがマルチモーダルAIが扱うデータの代表的な例！「画像」っていう目で見る情報（視覚情報モダリティって言うよ）と、「テキスト」っていう言葉の情報（言語情報モダリティって言うよ）を、セットでAIに勉強させるんだ。

例えばね、可愛いワンちゃんの写真（これが画像モダリティね）と、「公園で楽しそうにボールを追いかけてるゴールデンレトリバーだよ」（これがテキストモダリティね）っていう説明文がペアになってるデータを、たーくさんAIに見せるんだ。そうすると、AIはだんだん「あ、こういう画像には、こういう言葉がくっついてるんだな」とか「こういう言葉で説明されてるってことは、画像はこんな感じかな？」みたいに、画像と言葉の間の関係を理解できるようになるんだ。これができるようになると、AIに写真を見せたらその説明文を自動で作ってくれたり（画像キャプション生成）、逆に「青い空と白いヨットの絵を描いて」って言葉で頼んだら、本当にそういう絵を描いてくれたり（テキストからの画像生成）、写真を見ながら「この犬は何をしてるの？」って質問に答えてくれたり（視覚的な質問応答、VQAって言うよ）する、すごいことができるようになるんだ！

他の選択肢も見てみようか。
「ア) 大量の英語のテキストデータのみ。」これは言葉の情報だけだから、一つの情報（単一モダリティ）だね。
「イ) 様々な犬種の画像データのみ。」これも画像の情報だけだから、単一モダリティだ。
「エ) 金融市場の株価の時系列データのみ。」これも株価っていう数字の時間の流れだけだから、単一モダリティだね。

人間って、目で物を見たり、耳で音を聞いたり、言葉を話したり、いろんな感覚を使って世界を理解してるよね？マルチモーダルAIは、そんな人間みたいに、いろんな種類の情報を組み合わせて、もっと深く、もっと豊かに物事を理解できるAIを目指してるんだ。最近だと、声と映像を組み合わせた動画の理解とかも、マルチモーダルAIの研究テーマになってるよ。これからますます賢くなりそうだね！

---
**設問8**

深層強化学習が応用されている事例として、適切なものはどれか。

ア) 大量の顧客データに基づいたクレジットスコアリング。
イ) 医療画像からの腫瘍の自動検出。
ウ) 複雑なゲーム（囲碁やビデオゲームなど）における人間レベル、あるいはそれ以上のプレイスキルの獲得。
エ) 顧客のレビューテキストの感情分析。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ウ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「深層強化学習（しんそうきょうかがくしゅう）」、英語だとDeep Reinforcement Learning、略してDRL！これはね、「ディープラーニング（深層学習）」っていうAIの賢い脳みそと、「強化学習」っていうAIの賢い勉強方法を、ガッチャンコさせた、今すごく注目されてる技術なんだ。

「強化学習」っていうのは、AI（このAIのことを「エージェント」って言うよ）が、ある状況（これを「環境」って言うよ）の中で、自分でいろいろ試行錯誤しながら、「どうやったら一番ご褒美（報酬って言うよ）がたくさんもらえるかな？」っていうのを学習していくやり方なんだ。

「ウ) 複雑なゲーム（囲碁やビデオゲームなど）における人間レベル、あるいはそれ以上のプレイスキルの獲得。」これが深層強化学習がめちゃくちゃ得意なことの代表例！AlphaGo（アルファ碁）っていうAIが囲碁で世界トップクラスのプロ棋士に勝ったり、AlphaStar（アルファスター）っていうAIがStarCraft IIっていう超複雑なゲームで人間みたいに上手にプレイしたり、OpenAI FiveっていうAIチームがDota 2っていうゲームでプロの人間チームと互角以上に戦ったり…そういうニュース、聞いたことないかな？あれがまさに深層強化学習のすごいところ！これらのゲームではね、AIはゲームの盤面とか画面っていう、ものすごくたくさんの情報（これをディープラーニングで処理するんだ）を見て、次にどんな手を選ぶか（行動）を決める。そして、その結果、勝ったり点数が入ったりしたら（報酬）、その行動は「良かったんだな！」って学習していくんだ。

例えるなら、ロボットが迷路を脱出するゲームをやってるとしよう。最初はデタラメに動き回るんだけど、たまたまゴールにたどり着いたら「やったー！ご褒美ゲット！」ってなる。この「ご褒美」をもっとたくさんもらえるように、ロボットは「あ、壁にぶつからない方がいいな」とか「前にこの道を通ったらうまくいったぞ」みたいに、だんだん賢い動き方を学習していくんだ。このとき、ロボットが見てるカメラの映像（これが迷路の状況ね）から、「これは壁だ」「これは道だ」って理解するのに、ディープラーニングの力が使われるってわけ！

他の選択肢も見てみよう。
「ア) 大量の顧客データに基づいたクレジットスコアリング。」お客さんの情報から「この人にお金を貸しても大丈夫かな？」って判断するのは、主に「教師あり学習」（分類問題）の出番だね。過去のお客さんのデータと、ちゃんとお金を返してくれたかどうかの「正解ラベル」から学習するんだ。
「イ) 医療画像からの腫瘍の自動検出。」これも、お医者さんが「ここに腫瘍があるよ」って印をつけた画像データ（正解ラベル付きのデータ）から学習する「教師あり学習」（画像分類とか物体検出）がメインだね。
「エ) 顧客のレビューテキストの感情分析。」お客さんが書いたレビューが「すごく良い！」って言ってるのか、「うーん、イマイチ…」って言ってるのかを判断するのも、主に「教師あり学習」（テキスト分類）だよ。「このレビューはポジティブ」「このレビューはネガティブ」っていう正解ラベルがついた文章で学習するんだ。

深層強化学習はね、ゲーム以外にも、ロボットに複雑な手作業を教えたり、自動運転の車がどう動くべきか判断したり、工場で資源を一番効率よく使う方法を見つけたり…みたいに、「これが絶対の正解！」っていうお手本データがない状況で、試行錯誤しながら一番いいやり方を見つけ出す必要がある問題で、これからますます活躍しそうだね！

---
