# AI試験 模擬試験 - 8. AIに必要な数理・統計知識 (解答と解説)

### 問題1:
あるデータセット [2, 5, 5, 6, 7, 9] がある。このデータセットの中央値はどれか。

ア) 5
イ) 5.5
ウ) 6
エ) 5.67

**解答:** イ

**解説:**
中央値は、データを小さい順（または大きい順）に並べたときに、ちょうど真ん中に位置する値です。データの個数が偶数個の場合は、真ん中の2つの値の平均値を取ります。

1.  **データを小さい順に並べる:**
    [2, 5, 5, 6, 7, 9]
    すでに小さい順に並んでいます。

2.  **データの個数を確認する:**
    データは6個あります（偶数個）。

3.  **真ん中の2つの値を見つける:**
    データが6個の場合、真ん中の2つは、小さい方から数えて3番目と4番目の値です。
    3番目の値: 5
    4番目の値: 6

4.  **真ん中の2つの値の平均を計算する:**
    (5 + 6) / 2 = 11 / 2 = 5.5

したがって、このデータセットの中央値は5.5です。

*   **ア) 5:** これは3番目の値であり、中央値ではありません。
*   **ウ) 6:** これは4番目の値であり、中央値ではありません。
*   **エ) 5.67:** これは平均値 (2+5+5+6+7+9)/6 = 34/6 ≈ 5.67 です。中央値と平均値は異なる指標です。平均値は外れ値（極端に大きい値や小さい値）の影響を受けやすいですが、中央値は受けにくいという特徴があります。

### 問題2:
2つの確率変数XとYの共分散に関する記述として、最も適切なものはどれか。

ア) 共分散が正の場合、Xが増加するとYも増加する傾向がある。
イ) 共分散が0の場合、XとYは必ず独立である。
ウ) 共分散の値の範囲は、常に-1から1の間である。
エ) 共分散は、各変数の単位によらず常に同じ値をとる。

**解答:** ア

**解説:**
共分散は、2つの確率変数が一緒にどのように変動するか（つまり、一方が増加したときにもう一方が増加するのか、減少するのか、それとも関係ないのか）を示す指標です。

*   **ア) 共分散が正の場合、Xが増加するとYも増加する傾向がある。:** これが正しい説明です。共分散が正の大きな値を取るほど、XとYが一緒に増加する（または一緒に減少する）強い線形の関係があることを示唆します。
    *   例：一般的に、「勉強時間（X）」が増えれば「テストの点数（Y）」も上がる傾向があります。この場合、XとYの共分散は正になると考えられます。

*   **イ) 共分散が0の場合、XとYは必ず独立である。:** これは誤りです。共分散が0であることは、2つの変数の間に「線形な」関係がないことを示しますが、独立であるとは限りません。非線形な関係がある場合（例えば、U字型の関係）でも共分散が0になることがあります。「独立ならば共分散は0」は真ですが、その逆（「共分散が0ならば独立」）は必ずしも真ではありません。
*   **ウ) 共分散の値の範囲は、常に-1から1の間である。:** これは相関係数の範囲の説明です。共分散の値の範囲は、変数の単位に依存するため、特に定まっていません（理論的にはマイナス無限大からプラス無限大まで取り得ます）。
*   **エ) 共分散は、各変数の単位によらず常に同じ値をとる。:** これも誤りです。共分散は変数の単位に依存します。例えば、Xの単位がメートルからセンチメートルに変わると、共分散の値も変わってしまいます。この単位依存性をなくし、関係の強さを-1から1の範囲で標準化したものが「相関係数」です。

共分散は、2つの変数の関係の「方向性」と「強さ」の目安を与えますが、その絶対的な大きさだけでは解釈が難しいため、通常は相関係数と合わせて評価されます。

### 問題3:
ピアソンの積率相関係数に関する記述として、適切でないものはどれか。

ア) 値の範囲は-1から1までである。
イ) 2つの変数の間の線形関係の強さを示す指標である。
ウ) 相関係数が0に近い場合、2つの変数の間にはいかなる関係もないことを意味する。
エ) 相関係数が1に近い場合、強い正の相関があることを示す。

**解答:** ウ

**解説:**
ピアソンの積率相関係数（単に相関係数とも呼ばれます）は、2つの量的変数の間の直線的な関係の強さと方向を示す統計的指標です。

*   **ア) 値の範囲は-1から1までである。:** これは正しいです。相関係数は必ず-1以上1以下の値を取ります。
*   **イ) 2つの変数の間の線形関係の強さを示す指標である。:** これも正しいです。相関係数は、あくまで「直線的な」関係の強さを見るものであり、曲線的な関係（非線形関係）が強くても、相関係数が0に近くなることがあります。
*   **エ) 相関係数が1に近い場合、強い正の相関があることを示す。:** これも正しいです。相関係数が+1に近いほど、一方の変数が増加するともう一方の変数も直線的に増加する強い関係があることを意味します。逆に、-1に近い場合は、一方の変数が増加するともう一方の変数が直線的に減少する強い負の相関があることを意味します。

*   **ウ) 相関係数が0に近い場合、2つの変数の間にはいかなる関係もないことを意味する。:** これが適切でない記述です。相関係数が0に近いということは、2つの変数の間に「線形な」関係がほとんどないことを意味しますが、「いかなる関係もない」わけではありません。前述の通り、強い非線形な関係（例えば、お椀を伏せたような関係やU字型の関係）が存在する場合でも、相関係数は0に近くなることがあります。
    *   例：運動量と体脂肪率の関係を考えてみましょう。運動量が少なすぎても多すぎても体脂肪率が上がり、適度な運動量で体脂肪率が最も低くなるようなU字型の関係があったとします。この場合、全体として見ると直線的な関係は弱いため、相関係数は0に近くなる可能性がありますが、運動量と体脂肪率の間には明確な（非線形な）関係が存在しています。

相関係数を見る際には、散布図なども合わせて確認し、非線形な関係を見逃さないようにすることが重要です。

### 問題4:
「偏相関係数」を計算する主な目的は何か。

ア) 2つの変数の間の非線形な関係を調べるため。
イ) 3つ以上の変数がある場合に、他の変数の影響を除いた上での2変数間の相関関係を調べるため。
ウ) 名義尺度や順序尺度のような質的変数間の関連性を調べるため。
エ) 時系列データにおける自己相関を調べるため。

**解答:** イ

**解説:**
偏相関係数は、複数の変数が絡み合っている状況で、特定の2つの変数間の「真の」相関関係を見抜くために使われます。

*   **イ) 3つ以上の変数がある場合に、他の変数の影響を除いた上での2変数間の相関関係を調べるため。:** これが偏相関係数の主な目的です。通常の相関係数（ピアソンの積率相関係数など）は、2つの変数の間に他の変数が介在して影響を与えている場合（これを「見せかけの相関」または「擬似相関」と呼びます）でも、その影響を区別できません。偏相関係数は、他の特定の変数の影響を統計的に取り除いた（統制した）上で、注目する2変数間の純粋な相関の強さを評価します。
    *   例：「アイスクリームの売上」と「水難事故の発生件数」の間には、夏場になると両方とも増えるため、正の相関が見られるかもしれません。しかし、これは「気温の上昇」という共通の原因（第3の変数）が両者に影響しているためであり、アイスクリームの売上が直接水難事故を引き起こしているわけではありません。このような場合、気温の影響を取り除いて偏相関係数を計算すると、アイスクリームの売上と水難事故の発生件数の間の「真の」相関はほとんどなくなるはずです。

*   **ア) 2つの変数の間の非線形な関係を調べるため。:** 非線形な関係を調べるには、散布図の確認や、スピアマンの順位相関係数、ケンドールのタウなどの他の指標、あるいは非線形回帰などの手法が用いられます。
*   **ウ) 名義尺度や順序尺度のような質的変数間の関連性を調べるため。:** 質的変数間の関連性を調べるには、クラメールの連関係数、ファイ係数、カイ二乗検定などが用いられます。
*   **エ) 時系列データにおける自己相関を調べるため。:** 時系列データで、過去の自分自身の値と現在の値との間の相関（自己相関）を調べるには、自己相関係数や偏自己相関係数が用いられます。

偏相関係数は、多変量解析において、変数間の複雑な関係性をより深く理解するために役立ちます。

### 問題5:
あるサイコロを1回振るとき、出る目の期待値はどれか。ただし、各目が出る確率は等しく1/6とする。

ア) 3
イ) 3.5
ウ) 4
エ) 6

**解答:** イ

**解説:**
期待値は、確率的な事象において、平均的にどのくらいの値が得られるかを示す指標です。「（値）×（その値が出る確率）」を全ての場合について合計することで求められます。

サイコロの出る目（値）とその確率は以下の通りです。
*   1が出る確率: 1/6
*   2が出る確率: 1/6
*   3が出る確率: 1/6
*   4が出る確率: 1/6
*   5が出る確率: 1/6
*   6が出る確率: 1/6

期待値 E は以下のように計算されます。
E = (1 × 1/6) + (2 × 1/6) + (3 × 1/6) + (4 × 1/6) + (5 × 1/6) + (6 × 1/6)
E = (1 + 2 + 3 + 4 + 5 + 6) / 6
E = 21 / 6
E = 3.5

したがって、サイコロを1回振るときに出る目の期待値は3.5です。

*   **ア) 3:** 期待値より小さいです。
*   **ウ) 4:** 期待値より大きいです。
*   **エ) 6:** 最大の目であり、期待値ではありません。

サイコロの目自体は整数しかありませんが、期待値は平均的な値なので、このように小数になることもあります。「サイコロを何度も何度も振って出た目の平均を取ると、3.5に近づいていく」と解釈できます。

### 問題6:
「最小二乗法」が用いられる代表的な場面はどれか。

ア) クラスタリングにおいて、データ点を最も近いクラスタに割り当てる際。
イ) 回帰分析において、観測データとモデルによる予測値との誤差の二乗和を最小にするようにパラメータを決定する際。
ウ) 決定木において、最適な分割点を見つける際。
エ) ニューラルネットワークにおいて、活性化関数を選択する際。

**解答:** イ

**解説:**
最小二乗法は、測定データや観測データに対して、最もよく当てはまる（誤差が最も小さい）モデルのパラメータを決定するための古典的かつ強力な手法です。

*   **イ) 回帰分析において、観測データとモデルによる予測値との誤差の二乗和を最小にするようにパラメータを決定する際。:** これが最小二乗法が用いられる最も代表的な場面です。例えば、単回帰分析で y = ax + b という直線モデルをデータに当てはめるとき、実際の観測値 y<sub>i</sub> とモデルによる予測値 (ax<sub>i</sub> + b) との差（これを残差または誤差と呼びます）を計算します。この残差の「二乗和」が最小になるように、直線の傾き a と切片 b を決定するのが最小二乗法です。
    *   なぜ二乗するかというと、誤差がプラスでもマイナスでも二乗すれば正の値になり、また、大きな誤差により大きなペナルティを与える効果があるためです。
    *   例：たくさんの点の集まり（データ）があったとき、それらの点に最も「フィットする」直線を引こうとします。最小二乗法は、各点から直線までの縦方向の距離（誤差）を測り、その距離の二乗の合計が最も小さくなるような直線の引き方を見つける方法です。

*   **ア) クラスタリングにおいて、データ点を最も近いクラスタに割り当てる際。:** k-meansクラスタリングなどでは、各データ点を最も近いクラスタ中心（重心）に割り当て、クラスタ内誤差平方和を最小化しようとしますが、これは最小二乗法の直接的な応用というよりは、距離に基づく割り当てと重心計算の繰り返しです。
*   **ウ) 決定木において、最適な分割点を見つける際。:** 決定木では、不純度（ジニ係数やエントロピーなど）を最小にするように分割点を探します。
*   **エ) ニューラルネットワークにおいて、活性化関数を選択する際。:** 活性化関数の選択はモデル設計の一部であり、最小二乗法で決定されるものではありません。（ただし、ニューラルネットワークの学習において、誤差関数として平均二乗誤差が使われることはあり、その最小化を目指すという意味では関連はあります。）

最小二乗法は、その計算の簡便さや統計的な性質の良さから、科学や工学の様々な分野で広く利用されています。

### 問題7:
「最尤法（さいゆうほう）」の基本的な考え方として、最も適切なものはどれか。

ア) パラメータの事前分布を仮定し、観測データに基づいて事後分布を更新していく。
イ) 観測されたデータが得られる確率（尤度）を最大にするように、モデルのパラメータを推定する。
ウ) 損失関数を定義し、その損失を最小化するパラメータを探索する。
エ) 多数のモデルを学習させ、それらの予測を平均することで最終的な予測を得る。

**解答:** イ

**解説:**
最尤法（さいゆうほう、Maximum Likelihood Estimation, MLE）は、統計モデルのパラメータを推定するための非常に強力で一般的な方法です。

*   **イ) 観測されたデータが得られる確率（尤度）を最大にするように、モデルのパラメータを推定する。:** これが最尤法の基本的な考え方です。「尤度（ゆうど、Likelihood）」とは、「あるパラメータの値が与えられたときに、手元にある観測データが得られる確率（または確率密度）」のことです。最尤法では、この尤度が最も大きくなるように、つまり「今手元にあるデータが、最も起こりやすかった（最も尤もらしい）のは、パラメータがどのような値のときか？」という観点からパラメータを推定します。
    *   例：コインを10回投げて7回表が出たとします。このコインが表を出す確率（パラメータ p）はいくらだと考えるのが最も尤もらしいでしょうか？ もし p=0.1 だとしたら、10回中7回も表が出る確率は非常に低いです。もし p=0.9 でも、同様に7回表は少し不自然です。直感的には、p=0.7 のときが、10回中7回表という結果が最も「起こりやすそう」に感じられます。最尤法は、この「尤もらしさ」を数学的に最大化する p を見つけ出します。

*   **ア) パラメータの事前分布を仮定し、観測データに基づいて事後分布を更新していく。:** これはベイズ推定の考え方です。最尤法は事前分布を仮定しません（ただし、ベイズ推定における事後確率最大化(MAP)推定は、事前分布が一様分布の場合に最尤推定と一致します）。
*   **ウ) 損失関数を定義し、その損失を最小化するパラメータを探索する。:** 多くの機械学習アルゴリズム（最小二乗法も含む）がこのアプローチを取ります。最尤法も、負の対数尤度を損失関数とみなせば、損失最小化問題として捉えることができますが、基本的な考え方は「尤度の最大化」です。
*   **エ) 多数のモデルを学習させ、それらの予測を平均することで最終的な予測を得る。:** これはアンサンブル学習（バギングやブースティングなど）の考え方です。

最尤法は、様々な統計モデル（正規分布、二項分布、ポアソン分布など）や機械学習モデル（ロジスティック回帰など）のパラメータ推定に広く用いられています。

### 問題8:
データセット [10, 20, 30, 40, 50] がある。このデータセットの分散はどれか。

ア) 100
イ) 200
ウ) 250
エ) 30

**解答:** イ

**解説:**
分散は、データのばらつき具合を示す代表的な指標です。各データ点が平均値からどれだけ離れているかの二乗の平均で計算されます。

1.  **平均値を求める:**
    平均値 (μ) = (10 + 20 + 30 + 40 + 50) / 5
    μ = 150 / 5
    μ = 30

2.  **各データ点と平均値との差（偏差）を求める:**
    *   10 - 30 = -20
    *   20 - 30 = -10
    *   30 - 30 = 0
    *   40 - 30 = 10
    *   50 - 30 = 20

3.  **各偏差の二乗を求める:**
    *   (-20)<sup>2</sup> = 400
    *   (-10)<sup>2</sup> = 100
    *   (0)<sup>2</sup> = 0
    *   (10)<sup>2</sup> = 100
    *   (20)<sup>2</sup> = 400

4.  **偏差の二乗の合計を求める:**
    合計 = 400 + 100 + 0 + 100 + 400 = 1000

5.  **偏差の二乗の平均を求める（これが分散）:**
    分散 (σ<sup>2</sup>) = 合計 / データの個数
    σ<sup>2</sup> = 1000 / 5
    σ<sup>2</sup> = 200

したがって、このデータセットの分散は200です。

*   **ア) 100:** これは標準偏差（分散の平方根）です。√200 ≈ 14.14 なので、100ではありません。
*   **ウ) 250:** 計算間違いです。
*   **エ) 30:** これは平均値です。

分散が大きいほど、データが平均値の周りに広く散らばっていることを意味し、分散が小さいほど、データが平均値の周りに集まっていることを意味します。標準偏差（分散の正の平方根）も、ばらつきの指標としてよく用いられ、元のデータと同じ単位で解釈できるという利点があります。
（注：ここでは「母分散」の定義で計算しています。もし「不偏分散」（サンプルから母分散を推定する場合に用いる）を求める場合は、割る数を「データの個数 - 1」としますが、設問の意図としては定義通りの分散で良いでしょう。）
