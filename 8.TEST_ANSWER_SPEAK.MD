---
**設問1**

あるデータセット [2, 5, 5, 6, 7, 9] がある。このデータセットの中央値はどれか。

ア) 5
イ) 5.5
ウ) 6
エ) 5.67

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「中央値」って、テストの点数とかで「ちょうど真ん中の人の点数は何点かな？」みたいに、データを順番に並べたときに、ど真ん中に来る値のことなんだ。

まず、このデータセット [2, 5, 5, 6, 7, 9] を小さい順に並べてみよう。…おっと、もう並んでるね！ラッキー！

次に、データの個数を数えてみよう。1、2、3、4、5、6…うん、6個だね。
データの数が偶数個のときは、ど真ん中が二人になっちゃうから、その二人の平均を取るのがルールなんだ。

このデータだと、6個だから、真ん中の二人は小さい方から数えて3番目の「5」と、4番目の「6」だね。
さあ、この二人の平均を出してみよう！ (5 + 6) ÷ 2 = 11 ÷ 2 = 5.5 だね！

だから、このデータセットの中央値は「5.5」になるんだ。

ちなみに、選択肢の「エ) 5.67」っていうのは、全部の数字を足して個数で割った「平均値」に近い値だね。平均値もよく使うけど、中央値とはちょっと違うんだ。平均値は、ものすごーく大きい数字とか小さい数字（外れ値って言うよ）があると、そっちに引っ張られちゃうことがあるんだけど、中央値はそういうのに強いっていう特徴があるんだよ。覚えておくと便利かもね！

---
**設問2**

2つの確率変数XとYの共分散に関する記述として、最も適切なものはどれか。

ア) 共分散が正の場合、Xが増加するとYも増加する傾向がある。
イ) 共分散が0の場合、XとYは必ず独立である。
ウ) 共分散の値の範囲は、常に-1から1の間である。
エ) 共分散は、各変数の単位によらず常に同じ値をとる。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ア）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「共分散（きょうぶんさん）」って、なんだか難しそうな名前だけど、これはね、二つのものが一緒にどう変わっていくか、その関係の方向性を見るための道具なんだ。例えば、片方が増えたら、もう片方も増えるのかな？それとも減っちゃうのかな？はたまた、全然関係ない動きをするのかな？っていうのを探るんだ。

「ア) 共分散が正の場合、Xが増加するとYも増加する傾向がある。」これが正解！共分散がプラスの値になるっていうのは、Xが大きくなるとYも大きくなる、逆にXが小さくなるとYも小さくなる、みたいに、二つが同じ方向に動く傾向があるよーっていうサインなんだ。
例えばね、一般的に「勉強時間（X）」が増えれば、「テストの点数（Y）」も上がる傾向があるよね？こういう時、勉強時間とテストの点数の共分散はプラスになるって考えられるんだ。

他の選択肢も見てみようか。
「イ) 共分散が0の場合、XとYは必ず独立である。」これはね、ちょっと惜しいんだけど、間違いなんだ。「独立」っていうのは、二つが全くお互いに影響し合ってない状態のこと。共分散が0っていうのは、二つの間に「まっすぐな関係（線形な関係って言うよ）」がないですよーってことまでは言えるんだけど、例えばグルーンとカーブするような関係（非線形な関係）がある場合でも、共分散が0になっちゃうことがあるんだ。だから、「共分散が0でも、実は裏でコッソリ関係してるかもしれない」って覚えておこう。「独立なら共分散は0」は正しいんだけど、逆は必ずしも正しくないんだ。ややこしいけど大事なポイントだよ！
「ウ) 共分散の値の範囲は、常に-1から1の間である。」これはね、「相関係数」っていう別の道具の説明なんだ。共分散の値自体は、データの単位（例えば、身長がメートルなのかセンチメートルなのかとか）によって、いくらでも大きくなったり小さくなったりするから、-1から1の間って決まってるわけじゃないんだ。
「エ) 共分散は、各変数の単位によらず常に同じ値をとる。」これも間違い。さっき言ったみたいに、共分散はデータの単位が変わると、値も変わっちゃうんだ。だから、共分散の数字だけを見ても、「この二つの関係って、どれくらい強いんだろう？」っていうのがちょっと分かりにくいことがある。その弱点をカバーしてくれるのが、次に話す「相関係数」なんだよね。

というわけで、共分散は二つのものが一緒に動く「方向」と「強さ」のヒントをくれるけど、数字の大きさだけじゃ判断しにくいから、だいたい「相関係数」っていう相棒と一緒に使われることが多いんだ。覚えておいてね！

---
**設問3**

ピアソンの積率相関係数に関する記述として、適切でないものはどれか。

ア) 値の範囲は-1から1までである。
イ) 2つの変数の間の線形関係の強さを示す指標である。
ウ) 相関係数が0に近い場合、2つの変数の間にはいかなる関係もないことを意味する。
エ) 相関係数が1に近い場合、強い正の相関があることを示す。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ウ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「ピアソンの積率相関係数」、長い名前だね！単に「相関係数」って呼ばれることも多いよ。これはね、二つの数字のデータ（量的変数って言うよ）の間に、どれくらい「まっすぐな関係（線形関係）」があるか、その強さと方向を見るための、とっても便利な道具なんだ。

まず、正しい説明を見ていこう！
「ア) 値の範囲は-1から1までである。」これはバッチリ正しい！相関係数は、必ず-1から1までの間の数字になるんだ。
「イ) 2つの変数の間の線形関係の強さを示す指標である。」これもその通り！大事なのは「線形関係」ってところ。あくまで、データが直線みたいに並んでるかどうかを見てるんだ。
「エ) 相関係数が1に近い場合、強い正の相関があることを示す。」これも正しいね！相関係数がプラス1に近いほど、片方が増えたらもう片方もまっすぐ増えるっていう、強いプラスの関係があるってこと。逆に、マイナス1に近いほど、片方が増えたらもう片方がまっすぐ減るっていう、強いマイナスの関係があるってことだよ。

じゃあ、「ウ) 相関係数が0に近い場合、2つの変数の間にはいかなる関係もないことを意味する。」はどうかというと…これが「適切でない」んだな。相関係数が0に近いっていうのは、「二つのものの間に、まっすぐな関係はあんまりないみたいだね」ってことまでは言えるんだけど、「どんな関係も一切ない！」って断言しちゃうのは間違いなんだ。
なんでかっていうとね、さっきもちょっと話したけど、相関係数が見てるのはあくまで「まっすぐな」関係だけ。だから、例えばデータがUの字みたいにカーブしてたり、お椀をひっくり返したみたいに山なりになってたりするような「まっすぐじゃない関係（非線形関係）」が強くても、相関係数は0に近くなっちゃうことがあるんだ。

例えるなら、運動する時間と体脂肪率の関係とかどうかな？運動しなさすぎても体脂肪率は高いし、逆に運動しすぎても（例えば、体を壊すくらい無理したら）体脂肪率が上がっちゃうかもしれない。適度な運動がいちばん体脂肪率が低い、みたいなU字カーブの関係があったとしたら、全体で見るとまっすぐな関係じゃないから、相関係数は0に近くなるかもしれないけど、運動時間と体脂肪率の間にはちゃーんと関係があるよね？そういうことなんだ。

だから、相関係数を見るときは、それだけで判断しないで、できればデータの点をプロットした「散布図」っていうグラフも一緒に見て、「もしかして、まっすぐじゃないけど何か関係があるんじゃないかな？」って考えるのが大事なんだね！

---
**設問4**

「偏相関係数」を計算する主な目的は何か。

ア) 2つの変数の間の非線形な関係を調べるため。
イ) 3つ以上の変数がある場合に、他の変数の影響を除いた上での2変数間の相関関係を調べるため。
ウ) 名義尺度や順序尺度のような質的変数間の関連性を調べるため。
エ) 時系列データにおける自己相関を調べるため。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「偏相関係数（へんそうかんけいすう）」、これもまたちょっと難しい名前が出てきたね！これはね、世の中のいろんなものが複雑に絡み合ってる中で、本当に知りたい二つのものの間の「本当の絆の強さ」を見抜くための、探偵みたいな道具なんだ。

「イ) 3つ以上の変数がある場合に、他の変数の影響を除いた上での2変数間の相関関係を調べるため。」これが偏相関係数の一番大事な仕事！普通の相関係数って、二つのものの関係を見てるんだけど、実はその二つの関係って、裏で別の何かが糸を引いてる「見せかけの関係（擬似相関って言うよ）」かもしれないんだ。偏相関係数は、その「裏で糸を引いてるヤツ（第3の変数って言うよ）」の影響を、「ちょっと君は黙っててね」って感じで統計的に取り除いてあげて、本当に調べたい二つのものの間の、純粋な関係の強さを見るためのものなんだ。

例えばね、「アイスクリームの売上」と「プールでの水難事故の件数」って、夏になるとどっちも増えるから、データだけ見ると「アイスが売れると事故が増える！？」みたいに、プラスの相関が見えちゃうかもしれない。でもこれって、本当は「気温が上がる」っていう共通のボス（第3の変数）が、アイスの売上もプールの事故も両方増やしてるだけで、アイスが直接事故の原因じゃないよね？こういう時に、気温の影響を「はい、君はちょっと脇にいてね」って感じで取り除いて、アイスの売上と水難事故の「本当の」関係を見ると、たぶんほとんど関係ないって結果になるはず。これが偏相関係数の力なんだ！

他の選択肢も見てみよう。
「ア) 2つの変数の間の非線形な関係を調べるため。」まっすぐじゃない関係を見るのは、散布図とか、スピアマンの順位相関係数とか、また別の道具の仕事だね。
「ウ) 名義尺度や順序尺度のような質的変数間の関連性を調べるため。」「好き・嫌い」とか「満足・普通・不満」みたいな、数字じゃない質的なデータの関係を見るのは、クラメールの連関係数とか、カイ二乗検定とか、そういう道具を使うんだ。
「エ) 時系列データにおける自己相関を調べるため。」時間の流れに沿ったデータで、「今日の自分と昨日の自分はどれくらい似てるかな？」みたいに、過去の自分との関係（自己相関って言うよ）を見るのは、自己相関係数とか偏自己相関係数っていう、また別の専門家がいるんだ。

だから、偏相関係数っていうのは、たくさんのものが絡み合ってる中で、「本当にこの二つって関係あるの？それとも誰かのせいでそう見えるだけなの？」っていうのを見極めるのに役立つ、とっても賢い分析ツールなんだね！

---
**設問5**

あるサイコロを1回振るとき、出る目の期待値はどれか。ただし、各目が出る確率は等しく1/6とする。

ア) 3
イ) 3.5
ウ) 4
エ) 6

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「期待値」っていうのはね、くじ引きとかサイコロみたいに、何が出るか分からないもの（確率的な事象って言うよ）をやったときに、「平均して、だいたいどれくらいのものが手に入りそうかな？」っていうのを表す数字なんだ。「（もらえるもの）×（それがもらえる確率）」を、全部の可能性について足し合わせると計算できるよ。

この問題は、普通のサイコロを1回振るときだね。サイコロの目は1から6まであって、それぞれの目が出る確率は、どれも同じで1/6だって言ってるね。

じゃあ、計算してみよう！
*   1の目が出る確率は1/6だから、(1 × 1/6)
*   2の目が出る確率は1/6だから、(2 × 1/6)
*   3の目が出る確率は1/6だから、(3 × 1/6)
*   4の目が出る確率は1/6だから、(4 × 1/6)
*   5の目が出る確率は1/6だから、(5 × 1/6)
*   6の目が出る確率は1/6だから、(6 × 1/6)

これをぜーんぶ足し合わせるんだ。
期待値 = (1 × 1/6) + (2 × 1/6) + (3 × 1/6) + (4 × 1/6) + (5 × 1/6) + (6 × 1/6)
これは、分母が全部6だから、分子だけ足して最後に6で割ればいいね。
期待値 = (1 + 2 + 3 + 4 + 5 + 6) / 6
期待値 = 21 / 6
期待値 = 3.5

というわけで、サイコロを1回振ったときに出る目の期待値は「3.5」になるんだ！

「え？サイコロの目に3.5なんてないじゃん！」って思った？いいところに気づいたね！サイコロの目そのものは1とか2とか整数だけど、期待値っていうのは「平均するとこれくらい」っていう値だから、こんな風に小数になることもあるんだ。「もしサイコロを何回も何回もずーっと振り続けて、出た目の平均を取ったら、だんだん3.5っていう数字に近づいていくよ」って考えると分かりやすいかな。

だから、選択肢のアの3とか、ウの4とか、エの6は、期待値じゃないんだね。面白いよね！

---
**設問6**

「最小二乗法」が用いられる代表的な場面はどれか。

ア) クラスタリングにおいて、データ点を最も近いクラスタに割り当てる際。
イ) 回帰分析において、観測データとモデルによる予測値との誤差の二乗和を最小にするようにパラメータを決定する際。
ウ) 決定木において、最適な分割点を見つける際。
エ) ニューラルネットワークにおいて、活性化関数を選択する際。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「最小二乗法（さいしょうにじょうほう、または、さいしょうじじょうほう）」、これはね、データ分析の世界では昔から使われてる、とっても有名でパワフルな道具なんだ。何をする道具かっていうと、手元にあるデータ（観測データとか測定データって言うよ）に、一番「いい感じ」に当てはまるモデル（例えば、直線とか曲線とか）のパラメータ（モデルの形を決める数字のことね）を見つけ出すための方法なんだ。「いい感じ」っていうのは、つまり「誤差が一番小さい」ってことだよ。

「イ) 回帰分析において、観測データとモデルによる予測値との誤差の二乗和を最小にするようにパラメータを決定する際。」これが最小二乗法が一番活躍する場面！「回帰分析」っていうのは、例えば「身長（X）から体重（Y）を予測する」みたいに、ある数字から別の数字を予測するモデルを作ることだったよね。このとき、モデルは例えば「Y = aX + b」みたいな直線の式になったりする。最小二乗法はね、実際のデータ（例えば、たくさんの人の身長と体重のペア）と、AIが作った直線の式から予測した値との「ズレ」（これを「誤差」とか「残差」って言うよ）を計算するんだ。そして、この「ズレ」の「二乗」を全部足し合わせたものが、いっちばん小さくなるように、直線の傾き「a」とか切片「b」の値を決めてくれるんだ。

なんで「二乗」するかって？それはね、ズレがプラス方向でもマイナス方向でも、二乗すれば必ずプラスの数字になるでしょ？それに、ズレが大きいほど、二乗するともっともっと大きな数字になるから、「大きなズレは特にダメだよ！」って感じで、大きな誤差に強いペナルティを与える効果もあるんだ。賢いよね！

例えるなら、黒板にチョークで点がいくつかポツポツと打ってあるとするでしょ（これがデータね）。そこに、できるだけたくさんの点の近くを通るような、一番「いい感じ」の直線を一本引きたいとする。最小二乗法は、それぞれの点から直線までの縦の長さ（これが誤差）を測って、その長さの二乗を全部足したものが、一番小さくなるような直線の引き方（つまり、傾きと切片）を教えてくれる、そんなイメージだよ。

他の選択肢も見てみよう。
「ア) クラスタリングにおいて、データ点を最も近いクラスタに割り当てる際。」データをいくつかのグループに分ける「クラスタリング」っていう手法（例えばk-means法とか）では、それぞれのデータ点を一番近いグループの中心に割り当てたり、グループ内のデータのばらつきが一番小さくなるようにしたりするけど、これは最小二乗法そのものっていうよりは、距離を元にした割り当てと中心の計算を繰り返す感じだね。
「ウ) 決定木において、最適な分割点を見つける際。」「決定木」っていう、質問を繰り返して答えにたどり着くようなAIモデルでは、「不純度」（どれだけグループの中がごちゃ混ぜになってるかを表す指標）が一番小さくなるように、データの分け方を探すんだ。
「エ) ニューラルネットワークにおいて、活性化関数を選択する際。」AIの脳みその中で使われる「活性化関数」を選ぶのは、AIの設計の一部であって、最小二乗法で決めるものじゃないんだ。（ただね、ニューラルネットワークが勉強するときに、「誤差関数」として「平均二乗誤差」っていう、最小二乗法と考え方がそっくりなものを使うことはよくあるよ。だから、全く関係ないわけじゃないんだけどね！）

最小二乗法はね、計算が比較的簡単で、しかも統計的にも良い性質を持ってるから、科学の世界でも、工学の世界でも、本当にいろんなところで使われてる、まさに「縁の下の力持ち」なんだ！

---
**設問7**

「最尤法（さいゆうほう）」の基本的な考え方として、最も適切なものはどれか。

ア) パラメータの事前分布を仮定し、観測データに基づいて事後分布を更新していく。
イ) 観測されたデータが得られる確率（尤度）を最大にするように、モデルのパラメータを推定する。
ウ) 損失関数を定義し、その損失を最小化するパラメータを探索する。
エ) 多数のモデルを学習させ、それらの予測を平均することで最終的な予測を得る。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「最尤法（さいゆうほう）」、英語だと Maximum Likelihood Estimation、略してMLE！これも統計学とか機械学習の世界では超有名な、モデルのパラメータ（モデルの性格を決める数字のことね）を推定するための、とってもパワフルで一般的なやり方なんだ。

「イ) 観測されたデータが得られる確率（尤度）を最大にするように、モデルのパラメータを推定する。」これが最尤法のど真ん中の考え方！「尤度（ゆうど）」っていうのはね、ちょっと難しい言葉だけど、「もしモデルのパラメータがこういう値だったら、今手元にあるこのデータが実際に観察される確率（または確率っぽさ）はどれくらいかな？」っていうのを表すものなんだ。最尤法は、この「尤度」が一番大きくなるように、つまり「今目の前にあるこのデータが、一番『起こるべくして起きた！』って言えるのは、パラメータがどんな値のときだろう？」っていう視点から、一番もっともらしいパラメータの値を見つけ出すんだ。

例えるなら、コインを10回投げてみたら、7回も表が出たとしよう。「このコイン、どれくらい表が出やすいコインなんだろう？（これが推定したいパラメータだね）」って考えるとき、もしこのコインが「表が出る確率は10%しかないよ」っていうコインだとしたら、10回投げて7回も表が出るなんて、めちゃくちゃ珍しいことだよね？（つまり、尤度が低いってこと）。もし「表が出る確率は90%だよ」っていうコインだとしても、7回表っていうのはちょっと少ない気もする（これも尤度がそんなに高くないかも）。でも、もし「表が出る確率は70%だよ」っていうコインだとしたら、10回中7回表が出るっていうのは、なんだか一番「ありそう」な感じがしない？最尤法は、この「一番ありそう！」っていう感覚を、数学的に「尤度最大！」っていう形で計算して、一番もっともらしいパラメータ（この場合は、表が出る確率p=0.7）を見つけ出すんだ。

他の選択肢も見てみよう。
「ア) パラメータの事前分布を仮定し、観測データに基づいて事後分布を更新していく。」これはね、「ベイズ推定」っていう、また別の賢い推定方法の考え方なんだ。最尤法は、基本的には「このパラメータ、最初からこれくらいじゃないかな？」みたいな事前の予想（事前分布って言うよ）は使わないんだ。（ただし、ベイズ推定の中でも「MAP推定」っていうのは、事前分布が平らな場合（一様分布って言うよ）には、最尤推定と同じ結果になることがあるんだ。ちょっとややこしいけどね！）
「ウ) 損失関数を定義し、その損失を最小化するパラメータを探索する。」多くの機械学習のアルゴリズム（さっき出てきた最小二乗法もそうだね）は、この「間違い（損失）をできるだけ小さくしよう！」っていう考え方でパラメータを探すんだ。最尤法もね、実は「マイナスの対数尤度」っていうのを損失関数だと考えれば、結局は損失を最小にする問題と同じように扱えるんだけど、基本的な出発点は「尤度を最大にする！」っていうところなんだ。
「エ) 多数のモデルを学習させ、それらの予測を平均することで最終的な予測を得る。」これは、「アンサンブル学習」（バギングとかブースティングとかが有名だね）っていう、一人で頑張るよりチームで頑張った方がいい結果が出るよね！っていう考え方だね。

最尤法はね、正規分布とか二項分布とかポアソン分布みたいな有名な確率分布のパラメータを決めたり、ロジスティック回帰みたいな機械学習モデルのパラメータを決めたりするのに、本当にいろんなところで使われてる、とっても頼りになる方法なんだよ！

---
**設問8**

データセット [10, 20, 30, 40, 50] がある。このデータセットの分散はどれか。

ア) 100
イ) 200
ウ) 250
エ) 30

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「分散（ぶんさん）」っていうのはね、データがどれくらい「ばらついてるか」の度合いを表す、とっても代表的な数字なんだ。みんなが平均点の周りにキュッと集まってるのか、それとも平均点から遠ーく離れたところに広がってるのか、そういうのを見るんだね。計算方法はね、それぞれのデータが平均からどれだけ離れてるか（これを「偏差」って言うよ）を計算して、その偏差を二乗したものの平均を取るんだ。

じゃあ、このデータセット [10, 20, 30, 40, 50] で計算してみよう！

ステップ１：まずは平均値を求めよう！
平均値 = (10 + 20 + 30 + 40 + 50) ÷ 5
平均値 = 150 ÷ 5
平均値 = 30 だね！

ステップ２：次に、それぞれのデータが平均値の30からどれだけ離れてるか（偏差）を計算しよう！
*   10 の偏差は、10 - 30 = -20
*   20 の偏差は、20 - 30 = -10
*   30 の偏差は、30 - 30 = 0
*   40 の偏差は、40 - 30 = 10
*   50 の偏差は、50 - 30 = 20

ステップ３：今計算した偏差を、それぞれ二乗しよう！マイナスの符号も二乗したらプラスになるから気をつけてね！
*   (-20) × (-20) = 400
*   (-10) × (-10) = 100
*   0 × 0 = 0
*   10 × 10 = 100
*   20 × 20 = 400

ステップ４：この二乗したものを、ぜーんぶ足し合わせよう！
合計 = 400 + 100 + 0 + 100 + 400 = 1000 だね！

ステップ５：最後に、この合計をデータの個数（今回は5個だね）で割るんだ。これが分散だよ！
分散 = 1000 ÷ 5
分散 = 200

というわけで、このデータセットの分散は「200」になるんだ！

他の選択肢も見てみようか。
「ア) 100」これはね、もしかしたら標準偏差（分散の平方根だよ。√200 はだいたい14.14くらいだから、100じゃないね）と間違えちゃったかな？
「ウ) 250」うーん、これは計算のどこかで間違っちゃったかもしれないね。
「エ) 30」あ、これはさっき計算した平均値だね！分散じゃないよ。

分散の数字が大きいほど、データが平均の周りで広く散らばってるってこと。逆に、分散が小さいほど、データが平均の周りにキュッと集まってるってことなんだ。ちなみに、分散の平方根を取ったものを「標準偏差」って言って、これもばらつきを見るのによく使われるよ。標準偏差は、元のデータと同じ単位（例えば、点数とかなら「点」）で考えられるから、分かりやすいっていうメリットがあるんだ。

（ちなみにね、この問題では「母分散」っていう、手元にあるデータそのもののばらつきを計算したんだけど、もし手元のデータがもっと大きな集団の一部（サンプルって言うよ）で、その大きな集団全体のばらつきを推定したいときは、「不偏分散」っていって、最後に割る数を「データの個数 - 1」にするっていう、ちょっと違う計算をすることもあるんだ。でも、この問題は普通に分散を求めればOKだよ！）

---
