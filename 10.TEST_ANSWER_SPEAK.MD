---
**設問1**

AI倫理に関する国内外のガイドラインで共通して議論される事項として、適切でないものはどれか。

ア) プライバシーの保護
イ) AIモデルの学習速度の最大化
ウ) 公平性と差別禁止
エ) 透明性と説明責任

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

AIがどんどん賢くなって、私たちの生活に大きな影響を与えるようになってきたから、「AIって、ちゃんと人間や社会のためになるように使わないとね！」っていうルール作り（これを「AI倫理」って言うよ）が、世界中で真剣に考えられてるんだ。いろんな国や組織が「こういうことに気をつけようね！」っていうガイドラインを出してるんだけど、だいたい共通して「これは大事だよね！」って言われてるポイントがあるんだ。

まず、大事なポイントから見ていこう！
「ア) プライバシーの保護」。AIって、賢くなるためにたーくさんのデータを食べる（学習する）んだけど、そのデータの中に、みんなのプライベートな情報が含まれてることがあるよね。だから、その情報をちゃんと守って、勝手に使ったりしないように、そしてプライバシーを守るための特別な技術（例えば、データを誰のものか分からなくする「匿名化」とか、データから個人が特定されにくくする「差分プライバシー」とか）もちゃんと使おうね！って言われてるんだ。これは超重要だよね！

「ウ) 公平性と差別禁止」。AIがね、お手本にしたデータの中に、もしかしたら世の中の良くない偏見とか差別が隠れてるかもしれない。そうすると、AIもそれを真似しちゃって、例えば「男の子だからこうだ」とか「外国の人だからこうだ」みたいに、特定の人たちに対して不公平な判断をしちゃったり、差別を広めちゃったりするかもしれない。それは絶対ダメだよね！だから、AIはみんなに公平に接するように作らないといけないし、差別をなくすように努力しないといけないんだ。

「エ) 透明性と説明責任」。AIがどうしてそういう答えを出したのか、人間にもちゃんと分かるようにしようね（これが「透明性」）。そして、もしAIの判断で何か問題が起きたときには、誰がどう責任を取るのかハッキリさせようね（これが「説明責任」）。AIが「なんとなくこう思いました！」じゃ、困るもんね。ちゃんと理由を説明できないと、安心してAIに任せられないでしょ？

じゃあ、「イ) AIモデルの学習速度の最大化」はどうかな？AIが早く賢くなるのは、もちろん技術的には嬉しいことだけど、AI倫理のガイドラインで「とにかくスピードが一番大事！」なんてことは言わないんだ。むしろ、プライバシーをちゃんと守ったり、データに偏りがないかじっくりチェックしたり、そういう倫理的なことをしっかりやろうとすると、AIを作るのにちょっと時間がかかることもあるかもしれない。でも、倫理っていうのは、効率とか性能だけじゃ測れない、もっと大事な価値を守るためのものだからね。

他にもね、AI倫理のガイドラインでは、「人間の尊厳と、自分で考えて決める権利（自律性）を大事にしようね」とか、「AIは安全じゃないとダメだよね」「悪い人に使われないようにセキュリティも大事だよね」「地球環境のことも考えようね（持続可能性）」「何かあったときに、ちゃんと原因を突き止められるようにしようね（アカウンタビリティ）」みたいなことも、よく話し合われてるんだよ。

---
**設問2**

AIにおけるプライバシー問題について、データ収集段階と推論段階の双方で問題が生じうるとされている。推論段階でのプライバシー問題の例として、最も適切なものはどれか。

ア) 個人情報を本人の同意なく収集し、AIモデルの学習に利用する。
イ) AIモデルの出力から、学習データに含まれていた特定の個人の情報が推測できてしまう。
ウ) 監視カメラの映像をAIで解析し、個人の行動を追跡する。
エ) 匿名化が不十分なデータを学習に用いてしまう。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

AIとプライバシーの問題って、AIが作られてから使われるまで、いろんな場面で出てくる可能性があるんだ。この問題は、「推論段階」つまりAIが実際に「うーん、答えはこれだ！」って考えてるときのプライバシー問題はどれかな？って聞いてるね。

まず、AIのライフサイクルとプライバシー問題について、ちょっと整理してみようか。
「データ収集段階」っていうのは、AIがお勉強するためのお手本データを集めてくるところだね。
*   「ア) 個人情報を本人の同意なく収集し、AIモデルの学習に利用する。」これは、まさにデータ収集段階での大問題！勝手に人の情報を使っちゃダメだよね。個人情報保護法っていう法律にも違反しちゃうかもしれない。
*   「エ) 匿名化が不十分なデータを学習に用いてしまう。」これも収集段階とか、その後のデータお掃除（前処理って言うよ）段階の問題だね。「匿名化」っていうのは、データを誰のものか分からなくすることだけど、これが中途半端だと、他の情報と組み合わせたら結局誰かバレちゃった！なんてことになりかねない。これもプライバシー侵害だよね。

じゃあ、「推論段階」つまりAIが学習したことを使って、新しいデータについて何か判断したり予測したりしてる時に起こるプライバシー問題ってどんなのだろう？
*   「イ) AIモデルの出力から、学習データに含まれていた特定の個人の情報が推測できてしまう。」これが推論段階でのプライバシー問題の代表例！AIモデルって、実は学習したお手本データの情報を、思わぬ形で「記憶」しちゃってる場合があるんだ。そうすると、悪い人がAIにいろんな質問をしたり、AIが出した答えをじーっくり分析したりすることで、「ふむふむ、このAIの学習データには、あの有名人のAさんの病気の記録が含まれてたに違いない！」とか「このAIの学習データには、Bさんのあのヒミツの情報が入ってたな…」みたいに、元のお手本データの内容を推測できちゃうかもしれないんだ。これは怖いよね！「メンバーシップ推論攻撃」とか「属性推論攻撃」、「モデル反転攻撃」なんていう、専門的な名前もついてるんだ。

最後に、「ウ) 監視カメラの映像をAIで解析し、個人の行動を追跡する。」これは、AIを「使う」段階でのプライバシー問題だね。カメラの映像をAIが解析するっていうのは推論の一種だけど、その結果として個人の行動が丸裸にされちゃうのは、プライバシーの観点から大きな問題だよね。データ収集と推論、そして利用が一体になってるケースだね。設問は「推論段階でのプライバシー問題の例」をピンポイントで聞いてるから、イの方がより直接的かな。

だからね、AIが賢くなって答えを出す（推論する）ときにも、そのAIが昔勉強したお手本データの中身が、意図せず外に漏れ出しちゃう危険性があるってことなんだ。これを防ぐために、「差分プライバシー」みたいな、AIが個々のデータを覚えすぎないようにする技術も研究されてるんだよ。

---
**設問3**

総務省が公表している「カメラ画像利活用ガイドブック」の趣旨として、最も適切なものはどれか。

ア) カメラ画像のAI解析技術の向上を目的とした技術仕様書。
イ) カメラ画像の利活用におけるプライバシーや肖像権への配慮、関係者との合意形成の重要性などを示すもの。
ウ) 高性能な監視カメラの導入を推進するための補助金制度の案内。
エ) カメラ画像のデータ形式や保存期間に関する法的な強制力を持つ規制。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

街のあちこちやお店の中とか、最近本当にいろんなところにカメラがあって、その映像がAIで分析されたりして、私たちの生活に役立てられようとしてるよね。でも、それって「なんだかいつも見られてるみたいでイヤだな…」って思う人もいるかもしれない。便利さとプライバシー、どうバランスを取るかがすごく大事なんだ。

「イ) カメラ画像の利活用におけるプライバシーや肖像権への配慮、関係者との合意形成の重要性などを示すもの。」これが、総務省っていう国のお役所が出してる「カメラ画像利活用ガイドブック」の一番大事なポイント、つまり「趣旨（しゅし）」なんだ！このガイドブックはね、カメラの映像をビジネスとかで使いたい会社の人とか、町のために使いたい役所の人が、みんなのプライバシーとか、顔が勝手に使われない権利（肖像権って言うよ）をちゃんと守って、安心してカメラを使ってもらえるように、「こういうことに気をつけましょうね」「こういうやり方がいいですよ」っていうのを、分かりやすい例も出しながら教えてくれてるんだ。法律みたいに「絶対にこうしなさい！」って強制するもんじゃないんだけど、「みんなで気持ちよく技術を使えるように、自主的にこういう風に取り組んでみませんか？」っていうお願い、道しるべみたいなものだね。

特にね、ガイドブックでは「カメラを設置してますよー」ってちゃんとお知らせすることとか、「このカメラの責任者は私です！」ってハッキリさせることとか、「何かあったらここに連絡してくださいね」っていう窓口を作ることとか、集めたデータの管理をしっかりすることとか、何のためにカメラを使ってるのかをちゃんと説明すること（透明性の確保って言うよ）とかが、すごく大事だよって言ってるんだ。

他の選択肢も見てみよう。
「ア) カメラ画像のAI解析技術の向上を目的とした技術仕様書。」このガイドブックは、AIの技術的な教科書じゃないんだ。どっちかっていうと、技術を使うときの社会的なルールとか、倫理的な心構えに重点を置いてるんだね。
「ウ) 高性能な監視カメラの導入を推進するための補助金制度の案内。」「こういうカメラを買うとお金がもらえますよー」みたいな、補助金の話がメインじゃないんだ。
「エ) カメラ画像のデータ形式や保存期間に関する法的な強制力を持つ規制。」さっきも言ったけど、これはあくまで「ガイドブック」だから、「絶対にこうしなさい！」っていう法律の決まりじゃないんだ。もちろん、個人情報保護法とか、他の関係する法律はちゃんと守らないといけないけどね！

このガイドブックはね、カメラの映像を使うことで私たちの生活がもっと便利で安全になるようにしつつ、でも個人の権利とか気持ちもちゃんと大事にしようね、その両方がうまくいくことを目指してるんだ。いいバランスって大事だよね！

---
**設問4**

AIにおける「アルゴリズムバイアス」に関する説明として、最も適切なものはどれか。

ア) AIモデルが学習データに含まれる偏りを反映し、特定のグループに対して不公平な結果を出力する現象。
イ) AIアルゴリズムの計算効率が悪く、処理に時間がかかってしまう問題。
ウ) AIモデルが複雑すぎて、人間にはその動作原理を理解できない状態。
エ) 悪意のある第三者がAIシステムに不正アクセスし、アルゴリズムを改ざんすること。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（ア）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「アルゴリズムバイアス」または「AIバイアス」って呼ばれるものはね、AIが公平じゃなくなっちゃう、とっても大事な問題なんだ。

「ア) AIモデルが学習データに含まれる偏りを反映し、特定のグループに対して不公平な結果を出力する現象。」これがアルゴリズムバイアスのドンピシャな説明！AIモデルっていうのは、お手本になるデータ（学習データって言うよ）をたくさん見て、「世の中ってこういうものなんだな」ってパターンとか傾向を学ぶんだ。でもね、もしそのお手本データが、現実の社会にある良くない偏見とか差別をそのまま映し出しちゃってたり、特定の人たちのグループのデータがすごく少なかったり、逆に多すぎたりしたら、どうなると思う？そう、AIもその「偏り（バイアス）」をそのまま学習しちゃうんだ。その結果、AIが特定のグループの人たちに対して、不利な判断をしちゃったり、不正確な答えを出したりする、そういう不公平なことが起きちゃう。これがアルゴリズムバイアスなんだ。

例えばね、昔の会社の採用データで、たまたま男性の採用率が高くて、女性の採用率が低かったとしよう。もしそのデータでAI採用システムを作っちゃったら、AIは「ふむふむ、男性を多く採用するのがいいんだな」って間違って学習しちゃって、本当は優秀な女性の応募者を不当に低く評価しちゃうかもしれない。これは、元々のデータにあった偏りを、AIが増幅して「再生産」しちゃったってことだね。
もう一つ例を出すと、顔を見分けるAI（顔認識システム）を作るのに、お手本にした顔写真データの中に、特定の人種の人たちの顔があんまり入ってなかったら、そのAIは、その人種の人たちの顔を見分けるのがすごく苦手になっちゃうかもしれない。これもアルゴリズムバイアスの一例だね。

他の選択肢も見てみよう。
「イ) AIアルゴリズムの計算効率が悪く、処理に時間がかかってしまう問題。」これはAIのスピードの問題だから、アルゴリズムバイアスとはちょっと違うね。
「ウ) AIモデルが複雑すぎて、人間にはその動作原理を理解できない状態。」これはAIが「ブラックボックス」になっちゃってるっていう、AIの透明性とか解釈性の問題だね。アルゴリズムバイアスとは直接違うけど、AIがブラックボックスだと、中にどんなバイアスが隠れてるか見つけにくくなっちゃうっていう関係はあるかもしれないね。
「エ) 悪意のある第三者がAIシステムに不正アクセスし、アルゴリズムを改ざんすること。」これは、悪い人がAIにイタズラしたり、攻撃したりするっていうセキュリティの問題だね。アルゴリズムバイアスの普通の意味とは違うんだ。

アルゴリズムバイアスはね、AIが会社の人事とか、銀行のローン審査とか、もっと言うと裁判とか、そういう人の人生を左右するような大事な判断に使われるときに、ものすごく深刻な不公平とか差別を生み出しちゃう可能性があるんだ。だから、AIに隠れたバイアスがないか見つけたり、測ったり、そしてそれをできるだけ減らしたりするための技術とか、ルール作りの研究が、今すごく進められてるんだよ。

---
**設問5**

アルゴリズムバイアスが生じる原因として、適切でないものはどれか。

ア) 学習データにおける特定の属性（性別、人種など）の偏り。
イ) 社会に既存する差別や偏見がデータに反映されていること。
ウ) モデルの設計者が意図的に特定のグループを優遇または冷遇するような設計を行うこと。
エ) モデルの学習に使用するコンピュータの処理速度が十分に速いこと。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（エ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

AIに不公平な判断をさせちゃう「アルゴリズムバイアス」。これって、どうして生まれちゃうんだろうね？いろんな原因が複雑に絡み合ってるんだけど、この問題は「これはバイアスの原因じゃないよね？」っていうのを選ぶ問題だね。

まず、バイアスの主な原因を見てみよう！
「ア) 学習データにおける特定の属性（性別、人種など）の偏り。」これはバイアスの大きな原因の一つ（「サンプリングバイアス」って言ったりするよ）。例えば、AIに「これは犬の写真だよ」って教えるのに、柴犬の写真ばっかり見せて、プードルの写真はほんのちょっとしか見せなかったら、そのAIは柴犬のことはよーく分かるけど、プードルのことはあんまり得意じゃないAIになっちゃうかもしれないよね。それと同じで、特定の人たちのグループのデータが極端に少なかったり、逆に多すぎたりすると、AIはそのグループのことをちゃんと学べなくて、偏った判断をしやすくなっちゃうんだ。
「イ) 社会に既存する差別や偏見がデータに反映されていること。」これもすごく大事な原因（「歴史的バイアス」なんて言ったりするよ）。AIが勉強するデータって、結局は人間社会の活動の記録だったりすることが多いよね。もし、私たちの社会の中に、昔からの良くない差別とか、無意識の偏見とかがあったとしたら、そういうのがデータの中に「シミ」みたいに残っちゃってるかもしれない。そうすると、AIはそのシミまで「これが普通なんだ」って学習しちゃう可能性があるんだ。怖いよね。
「ウ) モデルの設計者が意図的に特定のグループを優遇または冷遇するような設計を行うこと。」これもバイアスの一つの原因になりうるね（「設計バイアス」って言うよ）。AIを作る人自身が、気づかないうちに自分の偏見をAIの設計とか、どんな情報（特徴量って言うよ）をAIに教えるかに反映しちゃったり、もっと悪いケースだと、わざと特定のグループが得したり損したりするようなAIを作っちゃうことも、理論的には考えられるよね（もちろん、これは倫理的に大問題だけどね！）。

じゃあ、「エ) モデルの学習に使用するコンピュータの処理速度が十分に速いこと。」はどうかというと…これがバイアスの原因とは直接関係ないんだな。コンピューターのスピードが速くても遅くても、お手本データやAIの設計に問題があったら、バイアスは生まれちゃう。むしろ、コンピューターのスピードが速いおかげで、もっとたくさんのデータを使えたり、もっと複雑なAIを作れるようになったりして、もしかしたら隠れてるバイアスを見つけたり、それに対処したりするチャンスが増えるかもしれないけど、スピード自体がバイアスの原因になるわけじゃないんだ。

他にもね、AIの成績を測る「ものさし」（評価指標って言うよ）の選び方がマズくて、結果的にバイアスを見逃しちゃったり（評価バイアス）、AIが出した答えが人々の行動を変えて、その変わった行動がまたAIのお手本データになっちゃう…っていう繰り返し（フィードバックループって言うよ）の中で、バイアスがどんどん大きくなっちゃうこともあるんだ。バイアスの原因って、本当にいろんなところに潜んでるんだね。

---
**設問6**

AI技術の悪用例である「ディープフェイク」とは何か、最も適切な説明はどれか。

ア) AIを用いて、実在しない人物の顔画像を大量に生成する技術。
イ) AIを用いて、既存の画像や動画に写っている人物の顔を別の人物の顔と入れ替えたり、実際には言っていないことを言っているかのように加工したりする技術。
ウ) AIを用いて、オンライン広告のクリック率を不正に操作する技術。
エ) AIを用いて、株価の予測モデルを不正に操作し、市場を混乱させる技術。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「ディープフェイク」って言葉、ニュースとかで聞いたことあるかな？これはね、AI、特にディープラーニングっていう賢いAIと、もの作りが得意なAI（GAN：敵対的生成ネットワークとかが有名だよ）の技術を、悪いことに使って作られた、本物そっくりのニセモノの画像とか動画のことなんだ。

「イ) AIを用いて、既存の画像や動画に写っている人物の顔を別の人物の顔と入れ替えたり、実際には言っていないことを言っているかのように加工したりする技術。」これがディープフェイクのドンピシャな説明！ものすごーく巧妙に作られたディープフェイクって、パッと見ただけじゃ本物と見分けがつかないくらいリアルなことがあるんだ。だから、これを使って、誰かの悪口を広めたり（名誉毀損）、プライベートな情報を勝手に暴露したり（プライバシー侵害）、ウソの情報を本当のことみたいに広めたり（偽情報の拡散）、選挙の時にライバル候補のニセ動画を作って評判を落としたり（政治的なプロパガンダ）…みたいに、いろんな悪いことに使われちゃう危険性が心配されてるんだ。

例えばね、有名な俳優さんとか、国の偉い政治家さんの顔を、全然別の人が映ってる動画にピタッと合成して、あたかもその有名人や政治家が、とんでもないことを言ったり、変な行動をしたりしてるかのような、ニセモノの動画を作れちゃう。これがディープフェイクの怖さなんだ。

他の選択肢も見てみよう。
「ア) AIを用いて、実在しない人物の顔画像を大量に生成する技術。」これはね、GANとかを使って「この世に存在しない人の顔」をAIにいっぱい作らせる技術（StyleGANとかが有名だよ）のことだね。ディープフェイクと技術的には近いけど、ディープフェイクっていうのは、主に「本当にいる人の顔」を悪用して、その人のフリをさせるケースを指すことが多いんだ。ちょっとニュアンスが違うね。
「ウ) AIを用いて、オンライン広告のクリック率を不正に操作する技術。」これは「アドフラウド」っていう、インターネット広告のズルだね。ディープフェイクとは違うよ。
「エ) AIを用いて、株価の予測モデルを不正に操作し、市場を混乱させる技術。」これは株価を操って儲けようとしたり、サイバー攻撃で経済をメチャクチャにしようとしたりする話で、ディープフェイクとは別物だね。

ディープフェイクの技術って、映画の特殊効果とか、ゲームのキャラクター作りとか、そういう面白いことに使える可能性もあるんだけど、やっぱり悪いことに使われるリスクがすごく高いから、「これは本物？それともニセモノ？」って見分ける技術の開発とか、法律でちゃんと取り締まれるようにするとか、みんなで「こういうのはダメだよね！」って話し合うことが、今すごく活発に行われてるんだ。

---
**設問7**

AIと環境保護に関する議論点として、シラバスで言及されている内容に合致するものはどれか。

ア) AIの導入による省エネルギー化が進み、環境負荷は常に低減される。
イ) 大規模なAIモデルの学習には大量の電力消費が伴う一方で、AIは気候変動の予測や対策にも活用されうる。
ウ) AI技術の発展は、環境問題とは全く関連性がない。
エ) 環境保護団体は、AI技術の利用に一貫して反対している。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

AI技術がどんどん賢くなって、いろんなところで使われるようになってるけど、それって地球の環境にとっては、いいことなのかな？それとも、もしかして悪いことなのかな？実はね、両方の側面があるんだ。

「イ) 大規模なAIモデルの学習には大量の電力消費が伴う一方で、AIは気候変動の予測や対策にも活用されうる。」これが、シラバスの内容にも合ってる、すごくバランスの取れた見方だね！

まず、ちょっと心配な「影」の側面から。特にね、最近話題の大規模言語モデル（LLM）みたいに、ものすごーく大きなAIモデルを賢くするためには（学習させるって言うよ）、超高性能なコンピューターをずーっと動かし続けないといけないんだ。そうすると、電気をめちゃくちゃたくさん使うことになる。もし、その電気が石炭とか石油を燃やして作られてる（化石燃料って言うよ）としたら、二酸化炭素がいっぱい出て、地球温暖化を進めちゃうかもしれない。これを「AIのカーボンフットプリント（二酸化炭素の足跡）」問題なんて言ったりするんだ。AIが賢くなるために、地球があったかくなっちゃったら困るよね。

でもね、AIには明るい「光」の側面もあるんだ！AI技術は、地球の環境問題を解決するのに役立つかもしれないって期待されてるんだよ。例えば、
*   天気とか気候のデータをAIで分析して、台風が来るのを早く予測したり、地球温暖化がこれからどうなるかをもっと正確に占ったりする。
*   太陽光発電とか風力発電みたいなクリーンなエネルギーを、もっと効率よく作れるようにAIがコントロールする。
*   街全体のエネルギー消費をAIが見張って、「ここの電気、使いすぎだよ！」とか「もっとこうすれば無駄が減るよ！」って教えてくれるスマートシティを実現する。
*   ジャングルの動物たちが悪い人に捕まえられないように（密猟って言うよ）、AIが見張り番をする。
こんな風に、AIが地球を守るヒーローになる可能性も秘めてるんだ！

他の選択肢も見てみよう。
「ア) AIの導入による省エネルギー化が進み、環境負荷は常に低減される。」「常に」とは限らないんだよね。さっき言ったみたいに、AIの使い方によっては、逆に電気をいっぱい使っちゃうこともあるからね。
「ウ) AI技術の発展は、環境問題とは全く関連性がない。」これも違うね。電気消費の面でも、環境保護に役立つ面でも、しっかり関係があるんだ。
「エ) 環境保護団体は、AI技術の利用に一貫して反対している。」これも一概には言えないな。環境を守る活動をしてる人たちも、AIが電気を使いすぎるのは心配しつつも、「AIって、もしかしたら地球を救うのに役立つかも！」って期待してる人もいるんだ。大事なのは、AIの良いところと悪いところをちゃんと見極めて、地球に優しい形でAI技術を育てて、使っていくことだよね。

だから、AIを作る人たちも、使う人たちも、「このAI、電気どれくらい使うのかな？」とか「もっとエコなAIって作れないかな？」とか「このAIで地球のために何かできないかな？」って、いつも地球のことを頭の片隅に置いておくのが、これからはすごく大事になってくるんだね！

---
**設問8**

AIガバナンスの取り組みの一つである「AI倫理アセスメント」の主な目的として、最も適切なものはどれか。

ア) AIシステムの開発コストを見積もること。
イ) AIシステムが倫理的・法的・社会的な問題を引き起こすリスクを事前に評価し、対策を講じること。
ウ) AIシステムのアルゴリズムの特許申請を行うこと。
エ) AIシステムに関わる全ての従業員の倫理観を統一すること。

**やあ、みんな！この問題、一緒に見ていこうか！**

**正解はね…（イ）なんだ！**

じゃあ、なんでそうなるのか、一緒に見ていこう！

「AIガバナンス」って、なんだかちょっと固い言葉だけど、これはね、「AIが暴走したり、悪いことに使われたりしないように、ちゃんと人間社会にとって良い子でいてもらうためのルール作りとか、見守り体制のこと」って考えると分かりやすいかな。その大事な取り組みの一つが「AI倫理アセスメント」なんだ。

「イ) AIシステムが倫理的・法的・社会的な問題を引き起こすリスクを事前に評価し、対策を講じること。」これがAI倫理アセスメントの一番大事な目的！新しいAIシステムを作ったり、どこかから導入したりする「前」に、「このAI、もしかして誰かのプライバシーを侵害しちゃうんじゃない？」とか「不公平な判断をして、誰かを差別しちゃったりしないかな？」とか「安全に使えなかったらどうしよう…」「法律に違反してないかな？」「これを使うことで、世の中の人がイヤな気持ちになったりしないかな？」みたいに、いろんな良くないこと（リスクって言うよ）が起きる可能性がないかどうかを、いろんな角度から事前にチェック（アセスメントって言うよ）するんだ。そして、もし「あ、これはちょっと危ないかも…」っていうリスクが見つかったら、それを避けるための方法を考えたり、リスクをできるだけ小さくするための対策をちゃんと実行したりするんだ。

例えるなら、新しい遊園地のアトラクションを作る前に、「このジェットコースター、お客さんが安全に楽しめるかな？」「途中で止まっちゃったりしないかな？」「もしもの時の避難経路は大丈夫かな？」って、いろんな専門家が集まって厳しくチェックするでしょ？AI倫理アセスメントも、それと似たようなイメージだね。AIという新しい乗り物が、みんなにとって安全で楽しいものになるように、事前にしっかり点検するんだ。

他の選択肢も見てみよう。
「ア) AIシステムの開発コストを見積もること。」AIを作るのにどれくらいお金がかかるか計算するのは、プロジェクト管理のお仕事だね。AI倫理アセスメントの主な目的じゃないんだ。でも、アセスメントの結果、「こういう対策が必要だね」ってなったら、その分お金が余計にかかることはあるかもしれないね。
「ウ) AIシステムのアルゴリズムの特許申請を行うこと。」AIのすごいアイデアを特許で守るのは、知的財産っていうまた別の話だね。
「エ) AIシステムに関わる全ての従業員の倫理観を統一すること。」AIを作る会社で働いてるみんなが、AI倫理についてちゃんと勉強したり、意識を高く持ったりするのはすごく大事だけど、アセスメントっていうのは、個々の「AIシステム」そのもののリスクを評価することに焦点を当ててるんだ。

AI倫理アセスメントはね、会社とか組織が「私たちはちゃんと責任を持ってAIを作って使ってますよ！」って社会に示して、みんなからの信頼を得て、AI技術の良いところを安心して楽しんでもらうために、問題が起きる前に先回りしてリスクを管理する、とっても大事なプロセスなんだ。ただチェックリストを埋めるだけじゃなくて、AIを使う人とか、AIの影響を受けるかもしれないいろんな立場の人たちの意見も聞きながら進めるのが、より良いアセスメントだって言われてるよ！

---
