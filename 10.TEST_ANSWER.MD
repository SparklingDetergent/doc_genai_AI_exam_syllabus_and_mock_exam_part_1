# AI試験 模擬試験 - 10. AI倫理・AIガバナンス (解答と解説)

### 問題1:
AI倫理に関する国内外のガイドラインで共通して議論される事項として、適切でないものはどれか。

ア) プライバシーの保護
イ) AIモデルの学習速度の最大化
ウ) 公平性と差別禁止
エ) 透明性と説明責任

**解答:** イ

**解説:**
AI技術が社会に大きな影響を与えるようになるにつれて、その倫理的な側面が重視され、国内外で様々なガイドラインが策定されています。これらのガイドラインには、共通して議論されるいくつかの重要な原則があります。

*   **ア) プライバシーの保護:** AIは大量のデータを学習に用いるため、個人のプライバシーを侵害しないようなデータの取り扱いや、プライバシーを保護する技術（例：差分プライバシー、匿名化）の利用が求められます。
*   **ウ) 公平性と差別禁止:** AIモデルが、学習データに含まれるバイアスを反映して、特定の属性（人種、性別、年齢など）を持つ人々に対して不公平な判断を下したり、差別を助長したりしないようにすることが重要です。
*   **エ) 透明性と説明責任:** AIの判断プロセスや根拠を人間が理解できるようにし（透明性）、AIの判断の結果に対して責任の所在を明確にすること（説明責任）が求められます。なぜAIがそのような結論に至ったのかを説明できなければ、AIを信頼して社会システムに組み込むことは難しくなります。

*   **イ) AIモデルの学習速度の最大化:** 学習速度の向上は技術的な目標の一つではありますが、AI倫理のガイドラインで中心的に議論される倫理原則ではありません。むしろ、倫理的な配慮（例えば、慎重なデータ収集やバイアスチェック）を行うことで、開発速度が多少犠牲になることもあり得ます。倫理は、効率性や性能だけでは測れない価値を重視します。

その他、AI倫理ガイドラインでは、「人間の尊厳と自律性の尊重」「安全性」「セキュリティ」「持続可能性（環境への配慮など）」「アカウンタビリティ（責任追跡可能性）」といった事項もよく議論されます。

### 問題2:
AIにおけるプライバシー問題について、データ収集段階と推論段階の双方で問題が生じうるとされている。推論段階でのプライバシー問題の例として、最も適切なものはどれか。

ア) 個人情報を本人の同意なく収集し、AIモデルの学習に利用する。
イ) AIモデルの出力から、学習データに含まれていた特定の個人の情報が推測できてしまう。
ウ) 監視カメラの映像をAIで解析し、個人の行動を追跡する。
エ) 匿名化が不十分なデータを学習に用いてしまう。

**解答:** イ

**解説:**
AIにおけるプライバシー問題は、AIのライフサイクルの様々な段階で発生し得ます。

*   **データ収集段階でのプライバシー問題:**
    *   **ア) 個人情報を本人の同意なく収集し、AIモデルの学習に利用する。:** これはデータ収集段階における典型的なプライバシー侵害です。個人情報保護法などに抵触する可能性があります。
    *   **エ) 匿名化が不十分なデータを学習に用いてしまう。:** 匿名化されていれば問題ないように見えても、他の情報と組み合わせることで個人が再特定できてしまう（再識別化）リスクがある場合、プライバシー問題となります。これも主に収集・前処理段階の問題です。

*   **推論段階でのプライバシー問題:**
    *   **イ) AIモデルの出力から、学習データに含まれていた特定の個人の情報が推測できてしまう。:** これが推論段階（AIモデルが実際に予測や判断を行う段階）でのプライバシー問題の例です。「メンバーシップ推論攻撃」や「属性推論攻撃」、「モデル反転攻撃」といった手法により、攻撃者がAIモデルの応答（出力）を分析することで、そのモデルの学習に使われたデータセットに特定の個人の情報が含まれていたかどうか、あるいはその個人の属性情報などを推測できてしまう可能性があります。
        *   例：ある病気の診断AIがあったとして、そのAIに様々な質問を投げかけることで、特定の有名人AさんがそのAIの学習データ（つまり、その病気の患者データ）に含まれていたかどうかを推測できてしまう、といった状況が考えられます。

*   **ウ) 監視カメラの映像をAIで解析し、個人の行動を追跡する。:** これはAIの「利用」段階におけるプライバシー問題であり、データ収集と一体となっていることが多いですが、設問の「推論段階での問題」という観点からは、イがより直接的な例と言えます。カメラ映像の解析自体が推論ですが、その結果（個人の行動履歴）がプライバシー侵害に繋がります。

推論段階でのプライバシー問題は、AIモデル自体が学習データに関する情報を「記憶」してしまい、それが意図せず外部に漏れ出てしまうリスクを示唆しています。差分プライバシーなどの技術は、このようなリスクを低減するために研究されています。

### 問題3:
総務省が公表している「カメラ画像利活用ガイドブック」の趣旨として、最も適切なものはどれか。

ア) カメラ画像のAI解析技術の向上を目的とした技術仕様書。
イ) カメラ画像の利活用におけるプライバシーや肖像権への配慮、関係者との合意形成の重要性などを示すもの。
ウ) 高性能な監視カメラの導入を推進するための補助金制度の案内。
エ) カメラ画像のデータ形式や保存期間に関する法的な強制力を持つ規制。

**解答:** イ

**解説:**
街中や店舗など、様々な場所でカメラが設置され、その画像データがAIなどで利活用される機会が増えています。これに伴い、プライバシー保護とのバランスが課題となっています。

*   **イ) カメラ画像の利活用におけるプライバシーや肖像権への配慮、関係者との合意形成の重要性などを示すもの。:** これが「カメラ画像利活用ガイドブック」の主な趣旨です。このガイドブックは、カメラ画像を事業などで利活用しようとする事業者や地方公共団体などが、プライバシーや肖像権に配慮し、生活者の不安を軽減するために、どのような点に注意すべきか、どのような対応が望ましいかを、具体的な事例も交えながら解説しています。法的な拘束力を持つものではありませんが、自主的な取り組みを促すための指針となっています。
    *   特に、設置の告知、管理責任者の明確化、問い合わせ窓口の設置、データの適切な管理、透明性の確保といった点が重要視されています。

*   **ア) カメラ画像のAI解析技術の向上を目的とした技術仕様書。:** ガイドブックは技術仕様書ではありません。技術的な側面よりも、社会的な受容性や倫理的・法的な配慮に重点を置いています。
*   **ウ) 高性能な監視カメラの導入を推進するための補助金制度の案内。:** 補助金制度に関する情報提供が主目的ではありません。
*   **エ) カメラ画像のデータ形式や保存期間に関する法的な強制力を持つ規制。:** ガイドブックはあくまで「指針」であり、法的な強制力を持つ「規制」ではありません。ただし、個人情報保護法などの関連法規を遵守する必要があることは当然です。

このガイドブックは、カメラ画像の利活用が社会に受け入れられ、技術の健全な発展と個人の権利利益の保護が両立することを目指しています。

### 問題4:
AIにおける「アルゴリズムバイアス」に関する説明として、最も適切なものはどれか。

ア) AIモデルが学習データに含まれる偏りを反映し、特定のグループに対して不公平な結果を出力する現象。
イ) AIアルゴリズムの計算効率が悪く、処理に時間がかかってしまう問題。
ウ) AIモデルが複雑すぎて、人間にはその動作原理を理解できない状態。
エ) 悪意のある第三者がAIシステムに不正アクセスし、アルゴリズムを改ざんすること。

**解答:** ア

**解説:**
アルゴリズムバイアス（またはAIバイアス）は、AIの公平性に関わる非常に重要な問題です。

*   **ア) AIモデルが学習データに含まれる偏りを反映し、特定のグループに対して不公平な結果を出力する現象。:** これがアルゴリズムバイアスの正しい説明です。AIモデルは、学習データに含まれるパターンや傾向を学習します。もし学習データが、現実社会に存在する偏見や差別を反映していたり、特定のグループのデータが不足していたり過剰だったりすると、AIモデルもそのバイアスを学習してしまい、結果として特定のグループに対して不利な、あるいは不正確な予測や判断をしてしまうことがあります。
    *   例1：過去の採用データで、特定の性別や人種の採用率が低かった場合、そのデータで学習したAI採用システムは、その性別や人種の人々を不当に低く評価してしまう可能性があります。これは、過去のデータに含まれるバイアスをAIが「再生産」してしまった例です。
    *   例2：顔認識システムで、特定の肌の色の人々の認識精度が著しく低い場合、学習データにその肌の色の人々の顔画像が十分にバランス良く含まれていなかったことが原因の一つとして考えられます。

*   **イ) AIアルゴリズムの計算効率が悪く、処理に時間がかかってしまう問題。:** これは計算性能の問題であり、アルゴリズムバイアスとは異なります。
*   **ウ) AIモデルが複雑すぎて、人間にはその動作原理を理解できない状態。:** これはAIの「ブラックボックス問題」や解釈性の問題であり、アルゴリズムバイアスとは直接的な意味合いが異なりますが、ブラックボックス性がバイアスの発見を難しくする要因にはなり得ます。
*   **エ) 悪意のある第三者がAIシステムに不正アクセスし、アルゴリズムを改ざんすること。:** これはセキュリティ侵害や敵対的攻撃の問題であり、アルゴリズムバイアスの通常の定義とは異なります。

アルゴリズムバイアスは、AIが社会的に重要な意思決定（採用、融資、司法など）に用いられる際に、深刻な不公平や差別を引き起こす可能性があるため、その検知、測定、緩和のための技術やガイドラインの研究が進められています。

### 問題5:
アルゴリズムバイアスが生じる原因として、適切でないものはどれか。

ア) 学習データにおける特定の属性（性別、人種など）の偏り。
イ) 社会に既存する差別や偏見がデータに反映されていること。
ウ) モデルの設計者が意図的に特定のグループを優遇または冷遇するような設計を行うこと。
エ) モデルの学習に使用するコンピュータの処理速度が十分に速いこと。

**解答:** エ

**解説:**
アルゴリズムバイアスは、様々な要因によってAIシステムに忍び込みます。

*   **ア) 学習データにおける特定の属性（性別、人種など）の偏り。:** これはバイアスの主要な原因の一つです（サンプリングバイアス）。例えば、ある属性を持つ人々のデータが極端に少なかったり、逆に多すぎたりすると、モデルはその属性に対して適切に一般化できず、偏った判断をしやすくなります。
*   **イ) 社会に既存する差別や偏見がデータに反映されていること。:** これも非常に重要な原因です（歴史的バイアス）。AIが学習するデータは、人間社会の活動の記録であることが多いです。もし社会に構造的な差別や無意識の偏見が存在すれば、それらがデータに「足跡」として残り、AIはその差別や偏見まで「学習」してしまう可能性があります。
*   **ウ) モデルの設計者が意図的に特定のグループを優遇または冷遇するような設計を行うこと。:** これもバイアスの一因となり得ます（設計バイアス）。設計者自身が持つ偏見がモデルの設計や特徴量の選択に影響を与えたり、あるいは特定の目的のために意図的にバイアスを組み込んだりすることも考えられます（これは倫理的に問題ですが）。

*   **エ) モデルの学習に使用するコンピュータの処理速度が十分に速いこと。:** コンピュータの処理速度は、バイアスの発生とは直接的な関係はありません。処理速度が速くても遅くても、学習データやモデル設計に問題があればバイアスは生じ得ます。むしろ、処理速度が速いことで、より大規模なデータや複雑なモデルを扱えるようになり、潜在的なバイアスを発見したり対処したりする機会が増える可能性はありますが、原因ではありません。

その他にも、評価指標の選択が不適切であったり（評価バイアス）、AIの出力がユーザーの行動に影響を与え、その結果がさらにAIの学習データになるというフィードバックループによってバイアスが増幅されたりする（フィードバックループバイアス）こともあります。

### 問題6:
AI技術の悪用例である「ディープフェイク」とは何か、最も適切な説明はどれか。

ア) AIを用いて、実在しない人物の顔画像を大量に生成する技術。
イ) AIを用いて、既存の画像や動画に写っている人物の顔を別の人物の顔と入れ替えたり、実際には言っていないことを言っているかのように加工したりする技術。
ウ) AIを用いて、オンライン広告のクリック率を不正に操作する技術。
エ) AIを用いて、株価の予測モデルを不正に操作し、市場を混乱させる技術。

**解答:** イ

**解説:**
ディープフェイクは、AI、特に深層学習（ディープラーニング）と生成技術（GAN：敵対的生成ネットワークなど）を悪用して作られる偽の画像や動画のことです。

*   **イ) AIを用いて、既存の画像や動画に写っている人物の顔を別の人物の顔と入れ替えたり、実際には言っていないことを言っているかのように加工したりする技術。:** これがディープフェイクの正しい説明です。非常に巧妙に作られたディープフェイクは、本物と見分けるのが困難な場合があり、個人の名誉毀損、プライバシー侵害、偽情報の拡散、政治的なプロパガンダなど、様々な悪用が懸念されています。
    *   例：有名人や政治家の顔を別の動画に合成して、あたかもその人が問題発言をしたり、不適切な行動をしたりしているかのような偽動画を作成する。

*   **ア) AIを用いて、実在しない人物の顔画像を大量に生成する技術。:** これはGANなどを使って「存在しない人の顔」を生成する技術（例：StyleGANなど）であり、ディープフェイクとは少しニュアンスが異なります。ディープフェイクは、主に「実在の人物」を対象に、その人の肖像を悪用するケースを指すことが多いです。ただし、技術的には関連しています。
*   **ウ) AIを用いて、オンライン広告のクリック率を不正に操作する技術。:** これはアドフラウド（広告詐欺）の一種であり、ディープフェイクとは異なります。
*   **エ) AIを用いて、株価の予測モデルを不正に操作し、市場を混乱させる技術。:** これは市場操作やサイバー攻撃の一形態であり、ディープフェイクとは異なります。

ディープフェイク技術は、エンターテイメント分野などでのポジティブな活用も考えられますが、その悪用リスクの高さから、検出技術の開発や法的な規制、倫理的な議論が活発に行われています。

### 問題7:
AIと環境保護に関する議論点として、シラバスで言及されている内容に合致するものはどれか。

ア) AIの導入による省エネルギー化が進み、環境負荷は常に低減される。
イ) 大規模なAIモデルの学習には大量の電力消費が伴う一方で、AIは気候変動の予測や対策にも活用されうる。
ウ) AI技術の発展は、環境問題とは全く関連性がない。
エ) 環境保護団体は、AI技術の利用に一貫して反対している。

**解答:** イ

**解説:**
AI技術の発展と普及は、環境保護の観点からも光と影の両側面を持っています。

*   **イ) 大規模なAIモデルの学習には大量の電力消費が伴う一方で、AIは気候変動の予測や対策にも活用されうる。:** これがシラバスの内容に合致する、バランスの取れた記述です。
    *   **負の側面（電力消費）:** 特に大規模なディープラーニングモデル（LLMなど）を学習させるためには、高性能なコンピュータを長時間稼働させる必要があり、膨大な量の電力を消費します。この電力消費が、化石燃料に依存する発電によって賄われる場合、二酸化炭素排出量の増加に繋がり、環境負荷を高めるという懸念があります（「AIのカーボンフットプリント」問題）。
    *   **正の側面（環境保護への活用）:** 一方で、AI技術は環境問題の解決にも貢献できる可能性があります。例えば、気象データや衛星画像をAIで解析して気候変動のパターンをより正確に予測したり、再生可能エネルギーの発電効率を最適化したり、スマートシティでエネルギー消費を効率化したり、生態系保護のために密猟を監視したりといった応用が考えられます。

*   **ア) AIの導入による省エネルギー化が進み、環境負荷は常に低減される。:** 「常に低減される」とは限りません。上記の電力消費の問題があるため、AIの利用方法によっては逆に環境負荷が増大する可能性もあります。
*   **ウ) AI技術の発展は、環境問題とは全く関連性がない。:** これは誤りです。上記のように、電力消費と活用の両面で関連があります。
*   **エ) 環境保護団体は、AI技術の利用に一貫して反対している。:** これも一概には言えません。環境保護団体も、AIの環境負荷を懸念しつつ、環境問題解決へのAI活用には期待を寄せている場合があります。重要なのは、AIのメリットとデメリットを比較衡量し、持続可能な形でAI技術を発展・利用していくことです。

AI開発者や利用者は、AIの環境への影響を意識し、よりエネルギー効率の高いモデルの開発、再生可能エネルギーの利用、環境問題解決への積極的な応用などを考えていく必要があります。

### 問題8:
AIガバナンスの取り組みの一つである「AI倫理アセスメント」の主な目的として、最も適切なものはどれか。

ア) AIシステムの開発コストを見積もること。
イ) AIシステムが倫理的・法的・社会的な問題を引き起こすリスクを事前に評価し、対策を講じること。
ウ) AIシステムのアルゴリズムの特許申請を行うこと。
エ) AIシステムに関わる全ての従業員の倫理観を統一すること。

**解答:** イ

**解説:**
AIガバナンスとは、AIが人間社会にとって望ましい形で開発・利用されるようにするためのルール作りや体制整備のことです。AI倫理アセスメントはその重要な手段の一つです。

*   **イ) AIシステムが倫理的・法的・社会的な問題を引き起こすリスクを事前に評価し、対策を講じること。:** これがAI倫理アセスメントの主な目的です。AIシステムを開発・導入する前に、そのシステムがプライバシー侵害、バイアスによる差別、安全性の問題、法的コンプライアンス違反、社会的な不信感といったネガティブな影響を引き起こす可能性がないかを多角的に評価（アセスメント）します。そして、特定されたリスクに対して、それを回避または軽減するための対策を検討・実施します。
    *   例：新しい顔認識システムを導入する前に、どのようなデータで学習したのか、誤認識率はどの程度か、誤認識した場合にどのような影響があるか、プライバシー保護は十分か、差別的な判断をしないか、などをチェックリストなどに基づいて評価し、問題があれば改善策を施したり、導入を見送ったりします。

*   **ア) AIシステムの開発コストを見積もること。:** コスト見積もりはプロジェクト管理の一部ですが、AI倫理アセスメントの主目的ではありません。ただし、アセスメントの結果、対策が必要になれば追加コストが発生することはあります。
*   **ウ) AIシステムのアルゴリズムの特許申請を行うこと。:** 特許戦略は知的財産管理の問題であり、倫理アセスメントとは異なります。
*   **エ) AIシステムに関わる全ての従業員の倫理観を統一すること。:** 従業員のAI倫理に関する教育や意識向上は重要ですが、アセスメントは個々の「システム」のリスク評価に焦点を当てます。

AI倫理アセスメントは、企業や組織が「責任あるAI」を実現し、社会からの信頼を得てAI技術の恩恵を享受するために、プロアクティブ（先を見越して行動する）にリスク管理を行うための重要なプロセスです。特定のチェックリストに従うだけでなく、多様なステークホルダー（利用者、影響を受ける可能性のある人々、専門家など）の意見を取り入れながら行うことが望ましいとされています。
