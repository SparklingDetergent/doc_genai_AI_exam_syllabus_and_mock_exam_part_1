# AI試験 模擬試験 - 5. ディープラーニングの要素技術 (解答と解説)

### 問題1:
畳み込みニューラルネットワーク (CNN) における「カーネル（フィルタ）」の主な役割として、最も適切なものはどれか。

ア) 入力データから特定の特徴（エッジ、テクスチャなど）を抽出する。
イ) 入力データの次元を削減し、計算量を減らす。
ウ) 活性化関数として機能し、非線形性を導入する。
エ) 出力層でクラスの確率を計算する。

**解答:** ア

**解説:**
カーネル（またはフィルタ）は、CNNの心臓部とも言える要素で、画像などの入力データから特徴を捉える役割を担います。

*   **ア) 入力データから特定の特徴（エッジ、テクスチャなど）を抽出する。:** これがカーネルの主な役割です。カーネルは、小さな数値の行列（例えば3x3ピクセル）で、これを入力画像の上でスライドさせながら畳み込み演算を行います。この演算により、画像中の特定の部分パターン（例えば、縦線、横線、特定の色の塊、特定の模様など）にカーネルが反応し、そのパターンの存在を示す「特徴マップ」が生成されます。
    *   例：虫眼鏡（カーネル）で絵（入力データ）の細部を順番に見ていき、特定の部分（例えば赤い丸）を見つけたら印をつける（特徴マップに出力する）ようなイメージです。異なる種類の虫眼鏡（異なるカーネル）を使えば、異なる特徴（青い四角など）を見つけることができます。

*   **イ) 入力データの次元を削減し、計算量を減らす。:** これは主にプーリング層の役割です。カーネル操作自体で次元が少し変わることはありますが、主目的ではありません。
*   **ウ) 活性化関数として機能し、非線形性を導入する。:** 活性化関数は畳み込み演算の後（またはバッチ正規化の後）に適用される別の要素であり、カーネル自体が活性化関数として機能するわけではありません。
*   **エ) 出力層でクラスの確率を計算する。:** これは主に全結合層とソフトマックス関数などの組み合わせで行われる処理であり、カーネルの直接的な役割ではありません。

CNNでは、様々な種類のカーネルが自動的に学習され、入力データから階層的に複雑な特徴を抽出していきます。

### 問題2:
CNNにおける「ストライド」の説明として、最も適切なものはどれか。

ア) 入力画像の外周に追加されるピクセルの幅。
イ) カーネルが一度に移動するピクセル数。
ウ) プーリング層で縮小される領域のサイズ。
エ) 畳み込み層の出力チャネル数。

**解答:** イ

**解説:**
ストライドは、CNNの畳み込み層やプーリング層において、カーネルやプーリングウィンドウが入力データ上をどれだけのステップで移動するかを指定するパラメータです。

*   **イ) カーネルが一度に移動するピクセル数。:** これがストライドの正しい説明です。ストライドが1の場合、カーネルは1ピクセルずつ移動します。ストライドが2の場合、カーネルは2ピクセルずつ移動します。
    *   例：床をモップで拭くとき、モップを1マスずつずらして拭けばストライド1、2マスずつずらして拭けばストライド2、というイメージです。ストライドを大きくすると、出力される特徴マップのサイズは小さくなります。

*   **ア) 入力画像の外周に追加されるピクセルの幅。:** これは「パディング」の説明です。パディングは、畳み込み処理で画像の端の情報が失われにくくしたり、出力サイズを調整したりするために使われます。
*   **ウ) プーリング層で縮小される領域のサイズ。:** これはプーリングウィンドウのサイズ（またはプールサイズ）の説明です。
*   **エ) 畳み込み層の出力チャネル数。:** これは使用するカーネルの数によって決まります。各カーネルが1つの出力チャネル（特徴マップ）を生成します。

ストライドの値を調整することで、特徴抽出の細かさや、出力される特徴マップのサイズをコントロールすることができます。

### 問題3:
プーリング層の主な目的として、適切でないものはどれか。

ア) 入力データに対する位置の微小なずれに対して頑健性を持たせる（不変性の獲得）。
イ) 特徴マップの次元を削減し、計算コストを低減する。
ウ) ネットワークのパラメータ数を大幅に増やすことで、表現力を向上させる。
エ) 過学習を抑制する効果が期待できる場合がある。

**解答:** ウ

**解説:**
プーリング層は、CNNにおいて畳み込み層の後によく挿入され、いくつかの重要な役割を果たします。

*   **ア) 入力データに対する位置の微小なずれに対して頑健性を持たせる（不変性の獲得）。:** これはプーリング層の重要な目的の一つです。例えば最大値プーリングでは、領域内の最大値を取るため、特徴の位置が多少ずれても同じ結果が出力されやすくなります。これにより、モデルが物体のわずかな位置変動に影響されにくくなります。
*   **イ) 特徴マップの次元を削減し、計算コストを低減する。:** これもプーリング層の主な目的です。プーリング処理によって特徴マップのサイズが小さくなるため、後続の層での計算量を大幅に削減できます。
*   **エ) 過学習を抑制する効果が期待できる場合がある。:** 次元削減により、モデルのパラメータ数を間接的に減らす効果があり、結果として過学習を抑制するのに役立つことがあります。

*   **ウ) ネットワークのパラメータ数を大幅に増やすことで、表現力を向上させる。:** これが適切でない記述です。プーリング層自体は学習すべきパラメータを持たない（あるいは非常に少ない）ため、パラメータ数を増やす効果はありません。むしろ、次元削減を通じて後続の層のパラメータ数を減らす方向に働きます。表現力の向上は主に畳み込み層や全結合層、そしてネットワーク全体の深さや幅によって達成されます。

プーリング層は、情報を「要約」することで、より本質的な特徴を保ちつつ、計算効率と汎化性能を高める役割を持つと理解できます。

### 問題4:
ResNetで導入された「スキップ結合（ショートカット接続）」が主に解決しようとした問題は何か。

ア) 畳み込み処理の計算コストが高い問題。
イ) ネットワークの層が深くなることによる勾配消失問題や性能劣化。
ウ) プーリング層による情報損失の問題。
エ) 活性化関数の選択が難しい問題。

**解答:** イ

**解説:**
ResNet（Residual Network）のスキップ結合は、深層学習の歴史における重要なブレークスルーの一つです。

*   **イ) ネットワークの層が深くなることによる勾配消失問題や性能劣化。:** これがスキップ結合が解決しようとした主要な問題です。理論的には、ニューラルネットワークは層を深くするほど複雑な関数を表現できるようになり、性能が向上するはずです。しかし、実際にはある程度以上層を深くすると、勾配が入力層近くまでうまく伝わらなくなる「勾配消失問題」や、訓練誤差・テスト誤差ともに悪化してしまう「劣化問題 (degradation problem)」が発生し、学習が困難になるという課題がありました。
    *   スキップ結合は、いくつかの層を飛び越えて入力を直接出力に足し合わせる経路（ショートカット）を作ります。これにより、勾配がこれらのショートカットを通じて直接的に浅い層にも伝わりやすくなり、また、ネットワークが少なくとも入力と同じ情報を出力する「恒等写像」を学習しやすくなるため、深いネットワークでも学習が容易になり、性能劣化を防ぐことができます。
    *   例：高層ビルのエレベーターが各階停止（通常の層の伝播）だけでなく、特定の階までノンストップで行ける急行運転（スキップ結合）も備えているようなイメージです。急行運転があることで、上層階（深い層）へのアクセスが容易になります。

*   **ア) 畳み込み処理の計算コストが高い問題。:** 計算コスト削減は、例えばプーリング層やストライドの調整、モデルの軽量化技術（量子化、プルーニングなど）のテーマであり、スキップ結合の主目的ではありません。
*   **ウ) プーリング層による情報損失の問題。:** プーリングによる情報損失は確かに存在しますが、スキップ結合が直接的にこれを解決するわけではありません。
*   **エ) 活性化関数の選択が難しい問題。:** 活性化関数の選択は重要ですが、スキップ結合はこれとは異なる課題に取り組んでいます。

ResNetのスキップ結合により、それまでは困難だった数百～千層といった非常に深いニューラルネットワークの学習が可能になり、画像認識などの分野で大幅な性能向上が達成されました。

### 問題5:
リカレントニューラルネットワーク (RNN) が特に適しているデータの種類は何か。

ア) 画像データのようなグリッド構造を持つデータ。
イ) 顧客の属性データのようなテーブル形式のデータ。
ウ) 音声やテキストのような時系列データやシーケンシャルデータ。
エ) 特徴間の関係性が複雑な高次元の疎なデータ。

**解答:** ウ

**解説:**
RNN (Recurrent Neural Network) は、データの「順序」や「時間的な連続性」を扱うのに適したニューラルネットワークのアーキテクチャです。

*   **ウ) 音声やテキストのような時系列データやシーケンシャルデータ。:** これがRNNが最も得意とするデータの種類です。RNNは、内部に「隠れ状態」と呼ばれるループ構造を持ち、過去の情報を記憶しながら現在の入力を処理することができます。これにより、単語の並び（テキスト）や音の連続（音声）、株価の推移（時系列データ）など、順序に意味があるデータのパターンを学習できます。
    *   例：文章を読むとき、私たちは前の単語の意味を記憶しながら次の単語を理解します。RNNはこの仕組みを模しており、「I am a student.」という文を読むとき、「I」「am」「a」という単語を順番に処理し、それぞれの単語の意味を考慮しながら文全体の意味を理解しようとします。

*   **ア) 画像データのようなグリッド構造を持つデータ。:** 画像データは主にCNN（畳み込みニューラルネットワーク）が得意とする分野です。CNNは空間的な局所性を捉えるのに長けています。ただし、画像キャプション生成（画像の内容を説明する文章を作る）のように、CNNで画像特徴を抽出した後にRNNで文章を生成する、といった組み合わせはあります。
*   **イ) 顧客の属性データのようなテーブル形式のデータ。:** テーブル形式のデータには、全結合型のニューラルネットワークや、決定木ベースのアルゴリズム（ランダムフォレスト、勾配ブースティングなど）がよく用いられます。データの順序性が重要でない場合はRNNの特性が活きにくいです。
*   **エ) 特徴間の関係性が複雑な高次元の疎なデータ。:** これだけではRNNが最適とは言えません。データの種類やタスクによります。

RNNは、その「記憶力」によって、過去の出来事が未来に影響を与えるような、時間的な依存関係を持つデータを扱うのに強力なツールとなります。

### 問題6:
LSTM (Long Short-Term Memory) や GRU (Gated Recurrent Unit) が、単純なRNNと比較して改善した主な点は何か。

ア) 計算速度を大幅に向上させた。
イ) より少ないデータで学習できるようになった。
ウ) 長期的な依存関係を学習しやすくなり、勾配消失問題を緩和した。
エ) あらゆる種類のデータに対応できるようになった。

**解答:** ウ

**解説:**
LSTMとGRUは、単純なRNN（Simple RNNとも呼ばれます）が抱える問題を解決するために考案された、より高性能なRNNのバリエーションです。

*   **ウ) 長期的な依存関係を学習しやすくなり、勾配消失問題を緩和した。:** これがLSTMやGRUの最大の改善点です。単純なRNNは、時系列を遡るほど過去の情報が薄れてしまい、長い時間離れた情報間の関係性（長期的な依存関係）を学習するのが苦手でした。これは、誤差逆伝播の際に勾配が消失したり爆発したりする問題が原因の一つです。
    *   LSTMやGRUは、「ゲート」と呼ばれる特殊な仕組みを導入しています。このゲートが、過去の情報を「記憶しておくべきか」「忘れるべきか」「新しく何を加えるべきか」を制御することで、必要な情報を長期にわたって保持しやすくなり、勾配消失問題も緩和されます。
    *   例：単純なRNNが、聞いたことをすぐに忘れてしまう短期記憶しか持たない人だとすると、LSTMやGRUは、重要なことだけを選んで長期記憶に保存できるメモ帳（ゲート機構）を持っている人のようなイメージです。これにより、長い文章の最初の方に出てきた重要な単語の意味を、文末まで覚えておくことができます。

*   **ア) 計算速度を大幅に向上させた。:** LSTMやGRUは内部構造が単純なRNNよりも複雑なため、一般的に計算コストは増加します。速度向上が主な改善点ではありません。
*   **イ) より少ないデータで学習できるようになった。:** データ量は依然として重要であり、LSTMやGRUが単純なRNNよりも劇的に少ないデータで済むわけではありません。
*   **エ) あらゆる種類のデータに対応できるようになった。:** LSTMやGRUもRNNの一種なので、得意なのはやはり時系列データやシーケンシャルデータです。対応できるデータの種類が根本的に変わったわけではありません。

LSTMやGRUの登場により、より長い文脈の理解や、より複雑な時系列パターンの学習が可能になり、機械翻訳、音声認識、文章生成などの分野で大きな進歩がありました。

### 問題7:
Transformerモデルで中心的な役割を果たす「Attention機構」に関する説明として、最も適切なものはどれか。

ア) 入力シーケンスの各要素に対して、均等な重みを割り当てることで情報を集約する。
イ) 出力を生成する際に、入力シーケンスの特定の部分に注目し、関連性の高い情報に大きな重みを割り当てる。
ウ) ネットワークの層をスキップすることで、情報の伝達を効率化する。
エ) 畳み込み演算を用いて、局所的な特徴を抽出する。

**解答:** イ

**解説:**
Attention（アテンション、注意）機構は、特にTransformerモデルの成功によって一躍有名になった、ディープラーニングにおける重要な概念です。

*   **イ) 出力を生成する際に、入力シーケンスの特定の部分に注目し、関連性の高い情報に大きな重みを割り当てる。:** これがAttention機構の核心的なアイデアです。長い入力シーケンス（例えば、長い文章）を処理する際に、出力に関連する重要な部分とそうでない部分があります。Attention機構は、出力の各要素を生成する際に、入力シーケンスのどの部分に「注意」を向けるべきかを動的に学習し、関連性の高い部分の情報に大きな重みを与えて利用します。
    *   例：英文を和訳するとき、日本語の一つの単語を訳出する際に、英文中の全ての単語が同じように重要なのではなく、特定のいくつかの単語が特に関連していることが多いです。Attention機構は、この「関連の深い単語はどれか」を自動で見つけ出し、その情報を重点的に使って翻訳を行います。

*   **ア) 入力シーケンスの各要素に対して、均等な重みを割り当てることで情報を集約する。:** これは単純な平均化などに近く、Attention機構の「どこに注目するかを選ぶ」という動的な重み付けの概念とは異なります。
*   **ウ) ネットワークの層をスキップすることで、情報の伝達を効率化する。:** これはスキップ結合（ResNetなど）の説明であり、Attention機構とは異なります。
*   **エ) 畳み込み演算を用いて、局所的な特徴を抽出する。:** これはCNNの説明です。Attention機構は、必ずしも局所的ではない、大域的な関連性も捉えることができます。

Transformerモデルは、このAttention機構を全面的に採用し、RNNを使わずに並列計算を可能にしながら、長期的な依存関係も捉えることに成功し、自然言語処理の分野に革命をもたらしました。

### 問題8:
データ拡張 (Data Augmentation) の主な目的として、最も適切なものはどれか。

ア) 学習データの量を擬似的に増やし、モデルの過学習を抑制して汎化性能を向上させる。
イ) 学習データのノイズを除去し、クリーンなデータセットを作成する。
ウ) 学習データの次元を削減し、計算効率を高める。
エ) モデルの解釈性を向上させるために、重要な特徴を強調する。

**解答:** ア

**解説:**
データ拡張は、手持ちの学習データを有効活用し、モデルの性能を向上させるための一般的なテクニックです。

*   **ア) 学習データの量を擬似的に増やし、モデルの過学習を抑制して汎化性能を向上させる。:** これがデータ拡張の主な目的です。機械学習モデル、特にディープラーニングモデルは、性能を十分に引き出すために大量の学習データを必要とします。しかし、十分な量のデータを集めるのが難しい場合もあります。データ拡張は、既存の学習データに少しずつ変化を加えた「新しい」データを人工的に作り出すことで、実質的な学習データ量を増やします。これにより、モデルがより多様なデータパターンに触れることができ、特定の訓練データに過剰に適合してしまう「過学習」を抑制し、未知のデータに対する予測能力（汎化性能）を高める効果が期待できます。
    *   例：猫の画像を学習させる場合、元の画像に対して、少し回転させたり、左右反転させたり、明るさを変えたり、一部を切り取ったりした画像を新たに生成します。これらは全て「猫」であることに変わりはありませんが、見た目が少しずつ異なるため、モデルはより多様な猫の姿を学習できます。

*   **イ) 学習データのノイズを除去し、クリーンなデータセットを作成する。:** データクリーニングやノイズ除去は別の重要な前処理ですが、データ拡張の主目的ではありません。データ拡張はむしろ、ある程度のノイズや変動に対するロバスト性（頑健性）をモデルに持たせる効果も期待できます。
*   **ウ) 学習データの次元を削減し、計算効率を高める。:** 次元削減は主成分分析(PCA)などの手法で行われ、データ拡張とは目的が異なります。
*   **エ) モデルの解釈性を向上させるために、重要な特徴を強調する。:** モデルの解釈性向上はXAIなどの分野のテーマであり、データ拡張の直接的な目的ではありません。

データ拡張は、比較的簡単に実行でき、多くの場合でモデル性能の向上に貢献するため、実務でもよく用いられるテクニックです。どのような拡張方法が有効かは、データの種類やタスクによって異なります。
